{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img title=\"GitHub Octocat\" src='./img/Octocat.jpg' style='height: 60px; padding-right: 15px' alt=\"Octocat\" align=\"left\"> This notebook is part of a GitHub repository: https://github.com/pessini/moby-bikes \n",
    "<br>MIT Licensed\n",
    "<br>Author: Leandro Pessini"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3\"></a>\n",
    "# <p style=\"font-size:100%; text-align:left; color:#444444;\">3- Exploratory Data Analysis (EDA)</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Hourly trend\n",
    "- Holiday\n",
    "- Working_day\n",
    "- Season"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Viz\n",
    "\n",
    "- Hour x Count by Season\n",
    "- Hour x Count by day of week\n",
    "- Hour x Count by temperature\n",
    "- Battery\n",
    "- Duration of rentals"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Rentals by Season"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 0 - Spring | 1 - Summer | 2 - Autumn | 3 - Winter\n",
    "season_map = {0:'Spring', 1:'Summer', 2:'Fall', 3:'Winter'}\n",
    "season_freq = hourly_data.groupby(['season', 'rental_hour'])['count'].agg('sum').reset_index(name='count')\n",
    "season_freq['season'] = season_freq['season'].map(lambda d : season_map[d])\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "sns.pointplot(x=season_freq[\"rental_hour\"], y=season_freq[\"count\"], hue=season_freq[\"season\"], ax=ax)\n",
    "ax.set(xlabel='Hour Of The Day', ylabel='Rentals Count', title=\"Number of Rentals By Hour Of The Day Across Seasons\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Season\n",
    "\n",
    "Summer data is low due to the last data collected is August 31st and the summer is yet not finished. **Decided to not use the season as a predictor** for now and after the changing of season the model will be rebuild adding this feature."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Rentals by Days of the Week"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "day_of_week_map = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\n",
    "day_of_week = hourly_data.groupby(['dayofweek', 'rental_hour'])['count'].agg('sum').reset_index(name='count')\n",
    "day_of_week['dayofweek'] = day_of_week['dayofweek'].map(lambda d : day_of_week_map[d])\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "sns.pointplot(x=day_of_week[\"rental_hour\"], y=day_of_week[\"count\"], hue=day_of_week[\"dayofweek\"], ax=ax)\n",
    "ax.set(xlabel='Hour Of The Day', ylabel='Rentals Count', title=\"Number of Rentals By Hour Of The Day Across Days of Week\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Battery distribution"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "battery_dist = all_data.copy()\n",
    "\n",
    "def group_battery_status(df):\n",
    "\n",
    "    bins= [0,30,50,80,100]\n",
    "    labels = ['< 30%','30% - 50%','50% - 80%','> 80%']\n",
    "    battery_dist['battery_status'] = pd.cut(battery_dist['start_battery'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    s = battery_dist.battery_status\n",
    "    counts = s.value_counts()\n",
    "    percent = s.value_counts(normalize=True)\n",
    "    percent100 = s.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n",
    "    \n",
    "    return pd.DataFrame({'counts': counts, 'per': percent, 'per100': percent100}, labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "group_battery_status(battery_dist)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.histplot(data=new_rentals, x='start_battery', kde=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_data['duration'].mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BoxPlot analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(nrows=2,ncols=2)\n",
    "fig.set_size_inches(12, 10)\n",
    "sns.boxplot(data=hourly_data,y=\"count\",orient=\"v\",ax=axes[0][0])\n",
    "sns.boxplot(data=hourly_data,y=\"count\",x=\"season\",orient=\"v\",ax=axes[0][1])\n",
    "sns.boxplot(data=hourly_data,y=\"count\",x=\"rental_hour\",orient=\"v\",ax=axes[1][0])\n",
    "sns.boxplot(data=hourly_data,y=\"count\",x=\"working_day\",orient=\"v\",ax=axes[1][1])\n",
    "\n",
    "axes[0][0].set(ylabel='Count',title=\"Box Plot On Count\")\n",
    "axes[0][1].set(xlabel='Season', ylabel='Count',title=\"Box Plot On Count Across Season\")\n",
    "axes[1][0].set(xlabel='Hour Of The Day', ylabel='Count',title=\"Box Plot On Count Across Hour Of The Day\")\n",
    "axes[1][1].set(xlabel='Working Day', ylabel='Count',title=\"Box Plot On Count Across Working Day\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hourly_data[hourly_data['count'] > 20]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Correlation Matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corrMatt = hourly_data[['temp','rain','wdsp','rhum','count']].corr()\n",
    "mask = np.array(corrMatt)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(8,8)\n",
    "sns.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.pairplot(hourly_data, x_vars=['temp','rain','wdsp','rhum'], y_vars='count', size=8, aspect=0.7, kind=\"reg\", palette='Set1')\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Distribution of numerical features"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(22,6))\n",
    "gs = fig.add_gridspec(1, 4)\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "ax3 = fig.add_subplot(gs[0, 3])\n",
    "\n",
    "sns.distplot(hourly_data['temp'],ax=ax0)\n",
    "sns.distplot(hourly_data['rain'],ax=ax1)\n",
    "sns.distplot(hourly_data['wdsp'],ax=ax2)\n",
    "sns.distplot(hourly_data['rhum'],ax=ax3)\n",
    "\n",
    "ax0.set(xlabel='Temperature',ylabel='Count',title=\"Distribution - Temperature\")\n",
    "ax1.set(xlabel='Rain', ylabel='Count',title=\"Distribution - Rain\")\n",
    "ax2.set(xlabel='Wind Speed', ylabel='Count',title=\"Distribution - Wind Speed\")\n",
    "ax3.set(xlabel='Relative Humidity', ylabel='Count',title=\"Distribution - Relative Humidity\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Transformation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "normalized_numerical_data = hourly_data.filter(['rain','wdsp','rhum'], axis=1)\n",
    "scaled_features = StandardScaler().fit_transform(normalized_numerical_data.values)\n",
    "scaled_features_df = pd.DataFrame(scaled_features, index=normalized_numerical_data.index, columns=normalized_numerical_data.columns)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "scaled_features_df.agg(['skew', 'kurtosis']).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hourly_data.temp.agg(['skew', 'kurtosis']).transpose()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Log transformation\n",
    "\n",
    "The logarithmic is a strong transformation that has a major effect on distribution shape. This technique is, as the square root method, oftenly used for reducing right skewness. Worth noting, however, is that it can not be applied to zero or negative values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Square Root\n",
    "The square root method is typically used when your data is moderately skewed. Now using the square root (e.g., sqrt(x)) is  a transformation that has a moderate effect on distribution shape. It is generally used to reduce right skewed data. Finally, the square root can be applied on zero values and is most commonly used on counted data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Box Cox Transformation\n",
    "The Box-Cox transformation is, as you probably understand, also a technique to transform non-normal data into normal shape. This is a procedure to identify a suitable exponent (Lambda = l) to use to transform skewed data."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(22,6))\n",
    "gs = fig.add_gridspec(1, 3)\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "sns.distplot(scaled_features_df['rain'],ax=ax0)\n",
    "sns.distplot(scaled_features_df['wdsp'],ax=ax1)\n",
    "sns.distplot(scaled_features_df['rhum'],ax=ax2)\n",
    "\n",
    "ax0.set(xlabel='Rain', ylabel='Count',title=\"Distribution - Rain\")\n",
    "ax1.set(xlabel='Wind Speed', ylabel='Count',title=\"Distribution - Wind Speed\")\n",
    "ax2.set(xlabel='Relative Humidity', ylabel='Count',title=\"Distribution - Relative Humidity\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Target feature (Count)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "transform_data = hourly_data.copy()\n",
    "transform_data['count'] = np.log(transform_data['count'] + 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "gs = fig.add_gridspec(1, 2)\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "sns.distplot(hourly_data['count'],ax=ax0)\n",
    "sns.distplot(transform_data['count'],ax=ax1)\n",
    "ax0.set(xlabel='Rentals Count',ylabel='Density',title=\"Distribution - Target Feature (Count)\")\n",
    "ax1.set(xlabel='Rentals Count',ylabel='Density',title=\"Distribution of Target variable after Log Transformation\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Outliers Analysis"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hourlyDataWithoutOutliers = hourly_data[np.abs(hourly_data[\"count\"]-hourly_data[\"count\"].mean())<=(2*hourly_data[\"count\"].std())] "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 8))\n",
    "sns.boxplot(data=hourlyDataWithoutOutliers,y=\"count\",orient=\"v\",ax=ax)\n",
    "ax.set(ylabel='Count',title=\"Box Plot On Count Withou Outliers\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.distplot(hourlyDataWithoutOutliers['count'],ax=ax)\n",
    "ax.set(xlabel='Rentals Count',ylabel='Density',title=\"Distribution - Target Feature (Without Outliers)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print (\"Shape of original data: \",hourly_data.shape)\n",
    "print (\"Shape of data without outliers: \",hourlyDataWithoutOutliers.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"4\"></a>\n",
    "# <p style=\"font-size:100%; text-align:left; color:#444444;\">4- Models</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting dataset in train and test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def adjusted_r2_score(X, r2_score):\n",
    "    return 1 - ( 1- r2_score ) * ( len(X) - 1 ) / ( len(X) - X.shape[1] - 1 )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = hourly_data.copy()\n",
    "#df = hourlyDataWithoutOutliers.copy()\n",
    "#df = transform_data.copy()\n",
    "df = df.astype({'holiday': 'category', 'dayofweek': 'category', 'working_day': 'category', 'peak': 'category'})\n",
    "predictors = ['rain','temp','wdsp','rhum','holiday','dayofweek','working_day','peak']\n",
    "\n",
    "X = df[[c for c in df.columns if c in predictors]]\n",
    "y = df.pop('count')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.shape, X_test.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split columns in categorical and numerical"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_vars = [c for c in df.select_dtypes(include=['number']).columns if c in predictors] # list comprehension to select only predictors features\n",
    "cat_vars = df.select_dtypes(include=['category']).columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing Pipelines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define categorical pipeline\n",
    "cat_pipe = Pipeline([\n",
    "    #('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "# Define numerical pipeline\n",
    "num_pipe = Pipeline([\n",
    "    #('imputer', SimpleImputer(strategy='median')),\n",
    "    #('scaler', StandardScaler())\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "# Combine categorical and numerical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_pipe, cat_vars),\n",
    "    ('num', num_pipe, num_vars)\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Feature Importance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# LightGBM model\n",
    "params_lightgbm = {'n_estimators': 500, 'boosting_type': 'dart', 'max_depth': 5, 'learning_rate': 0.01, 'subsample': 0.7, 'importance_type': 'gain'}\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_gbm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LGBMRegressor(**params_lightgbm))\n",
    "])\n",
    "pipe_gbm.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Plotting features importance\n",
    "feature_imp = pd.DataFrame(sorted(zip(pipe_gbm['model'].feature_importances_,X_train.columns)), columns=['Value','Feature'])\n",
    "scaler_ft = MinMaxScaler()\n",
    "feature_imp['Value'] = scaler_ft.fit_transform(feature_imp['Value'].values.reshape(-1,1))\n",
    "\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('LightGBM Features Importance')\n",
    "locs, labels = plt.xticks()\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pipe_linear_regression = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "pipe_linear_regression.fit(X_train, y_train)\n",
    "\n",
    "# Predict training data\n",
    "y_train_pred = pipe_linear_regression.predict(X_train)\n",
    "\n",
    "# Predict test data\n",
    "y_test_pred = pipe_linear_regression.predict(X_test)\n",
    "\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2:', metrics.r2_score(y_test, y_test_pred))\n",
    "print('Adjusted R^2:', adjusted_r2_score(X_test, metrics.r2_score(y_test, y_test_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Linear Regression Assumptions\n",
    "\n",
    "- Linearity\n",
    "- Normality of the Error\n",
    "- No Multicollinearity among Predictors\n",
    "- No Autocorrelation of the Error Terms\n",
    "- Homoscedasticity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing Linear Regression Assumptions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Returning the R^2 for the model\n",
    "linear_r2 = pipe_linear_regression.score(X_train, y_train)\n",
    "print('R^2: {0}'.format(linear_r2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def calculate_residuals(model, features, label):\n",
    "    \"\"\"\n",
    "    Creates predictions on the features with the model and calculates residuals\n",
    "    \"\"\"\n",
    "    predictions = model.predict(features)\n",
    "    df_results = pd.DataFrame({'Actual': label, 'Predicted': predictions})\n",
    "    df_results['Residuals'] = abs(df_results['Actual']) - abs(df_results['Predicted'])\n",
    "    \n",
    "    return df_results"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linearity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def linear_assumption(model, features, label):\n",
    "    \"\"\"\n",
    "    Linearity: Assumes that there is a linear relationship between the predictors and\n",
    "               the response variable. If not, either a quadratic term or another\n",
    "               algorithm should be used.\n",
    "    \"\"\"\n",
    "    print('Assumption 1: Linear Relationship between the Target and the Feature', '\\n')\n",
    "        \n",
    "    print('Checking with a scatter plot of actual vs. predicted.',\n",
    "           'Predictions should follow the diagonal line.')\n",
    "    \n",
    "    # Calculating residuals for the plot\n",
    "    df_results = calculate_residuals(model, features, label)\n",
    "    \n",
    "    # Plotting the actual vs predicted values\n",
    "    sns.lmplot(x='Actual', y='Predicted', data=df_results, fit_reg=False, size=7)\n",
    "        \n",
    "    # Plotting the diagonal line\n",
    "    line_coords = np.arange(df_results.min().min(), df_results.max().max())\n",
    "    plt.plot(line_coords, line_coords,  # X and y points\n",
    "             color='darkorange', linestyle='--')\n",
    "    plt.title('Actual vs. Predicted')\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "linear_assumption(pipe_linear_regression, X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normality of the Error Terms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def normal_errors_assumption(model, features, label, p_value_thresh=0.05):\n",
    "    \"\"\"\n",
    "    Normality: Assumes that the error terms are normally distributed. If they are not,\n",
    "    nonlinear transformations of variables may solve this.\n",
    "               \n",
    "    This assumption being violated primarily causes issues with the confidence intervals\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.diagnostic import normal_ad\n",
    "    print('Assumption 2: The error terms are normally distributed', '\\n')\n",
    "    \n",
    "    # Calculating residuals for the Anderson-Darling test\n",
    "    df_results = calculate_residuals(model, features, label)\n",
    "    \n",
    "    print('Using the Anderson-Darling test for normal distribution')\n",
    "\n",
    "    # Performing the test on the residuals\n",
    "    p_value = normal_ad(df_results['Residuals'])[1]\n",
    "    print('p-value from the test - below 0.05 generally means non-normal:', p_value)\n",
    "    \n",
    "    # Reporting the normality of the residuals\n",
    "    if p_value < p_value_thresh:\n",
    "        print('Residuals are not normally distributed')\n",
    "    else:\n",
    "        print('Residuals are normally distributed')\n",
    "    \n",
    "    # Plotting the residuals distribution\n",
    "    plt.subplots(figsize=(12, 6))\n",
    "    plt.title('Distribution of Residuals')\n",
    "    sns.distplot(df_results['Residuals'])\n",
    "    plt.show()\n",
    "    \n",
    "    print()\n",
    "    if p_value > p_value_thresh:\n",
    "        print('Assumption satisfied')\n",
    "    else:\n",
    "        print('Assumption not satisfied')\n",
    "        print()\n",
    "        print('Confidence intervals will likely be affected')\n",
    "        print('Try performing nonlinear transformations on variables')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "normal_errors_assumption(pipe_linear_regression, X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## No Multicollinearity among Predictors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def multicollinearity_assumption(model, features, label, feature_names=None):\n",
    "    \"\"\"\n",
    "    Multicollinearity: Assumes that predictors are not correlated with each other. If there is\n",
    "                       correlation among the predictors, then either remove prepdictors with high\n",
    "                       Variance Inflation Factor (VIF) values or perform dimensionality reduction\n",
    "                           \n",
    "                       This assumption being violated causes issues with interpretability of the \n",
    "                       coefficients and the standard errors of the coefficients.\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    print('Assumption 3: Little to no multicollinearity among predictors')\n",
    "        \n",
    "    # Plotting the heatmap\n",
    "    plt.figure(figsize = (10,8))\n",
    "    sns.heatmap(pd.DataFrame(features, columns=feature_names).corr(), annot=True)\n",
    "    plt.title('Correlation of Variables')\n",
    "    plt.show()\n",
    "        \n",
    "    print('Variance Inflation Factors (VIF)')\n",
    "    print('> 10: An indication that multicollinearity may be present')\n",
    "    print('> 100: Certain multicollinearity among the variables')\n",
    "    print('-------------------------------------')\n",
    "       \n",
    "    # Gathering the VIF for each variable\n",
    "    VIF = [variance_inflation_factor(features, i) for i in range(features.shape[1])]\n",
    "    for idx, vif in enumerate(VIF):\n",
    "        print('{0}: {1}'.format(feature_names[idx], vif))\n",
    "        \n",
    "    # Gathering and printing total cases of possible or definite multicollinearity\n",
    "    possible_multicollinearity = sum([1 for vif in VIF if vif > 10])\n",
    "    definite_multicollinearity = sum([1 for vif in VIF if vif > 100])\n",
    "    print()\n",
    "    print('{0} cases of possible multicollinearity'.format(possible_multicollinearity))\n",
    "    print('{0} cases of definite multicollinearity'.format(definite_multicollinearity))\n",
    "    print()\n",
    "\n",
    "    if definite_multicollinearity == 0:\n",
    "        if possible_multicollinearity == 0:\n",
    "            print('Assumption satisfied')\n",
    "        else:\n",
    "            print('Assumption possibly satisfied')\n",
    "            print()\n",
    "            print('Coefficient interpretability may be problematic')\n",
    "            print('Consider removing variables with a high Variance Inflation Factor (VIF)')\n",
    "\n",
    "    else:\n",
    "        print('Assumption not satisfied')\n",
    "        print()\n",
    "        print('Coefficient interpretability will be problematic')\n",
    "        print('Consider removing variables with a high Variance Inflation Factor (VIF)')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "multicollinearity_assumption(pipe_linear_regression, X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## No Autocorrelation of the Error Terms"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def autocorrelation_assumption(model, features, label):\n",
    "    \"\"\"\n",
    "    Autocorrelation: Assumes that there is no autocorrelation in the residuals. If there is\n",
    "                     autocorrelation, then there is a pattern that is not explained due to\n",
    "                     the current value being dependent on the previous value.\n",
    "                     This may be resolved by adding a lag variable of either the dependent\n",
    "                     variable or some of the predictors.\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.stattools import durbin_watson\n",
    "    print('Assumption 4: No Autocorrelation', '\\n')\n",
    "    \n",
    "    # Calculating residuals for the Durbin Watson-tests\n",
    "    df_results = calculate_residuals(model, features, label)\n",
    "\n",
    "    print('\\nPerforming Durbin-Watson Test')\n",
    "    print('Values of 1.5 < d < 2.5 generally show that there is no autocorrelation in the data')\n",
    "    print('0 to 2< is positive autocorrelation')\n",
    "    print('>2 to 4 is negative autocorrelation')\n",
    "    print('-------------------------------------')\n",
    "    durbinWatson = durbin_watson(df_results['Residuals'])\n",
    "    print('Durbin-Watson:', durbinWatson)\n",
    "    if durbinWatson < 1.5:\n",
    "        print('Signs of positive autocorrelation', '\\n')\n",
    "        print('Assumption not satisfied')\n",
    "    elif durbinWatson > 2.5:\n",
    "        print('Signs of negative autocorrelation', '\\n')\n",
    "        print('Assumption not satisfied')\n",
    "    else:\n",
    "        print('Little to no autocorrelation', '\\n')\n",
    "        print('Assumption satisfied')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "autocorrelation_assumption(pipe_linear_regression, X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Homoscedasticity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def homoscedasticity_assumption(model, features, label):\n",
    "    \"\"\"\n",
    "    Homoscedasticity: Assumes that the errors exhibit constant variance\n",
    "    \"\"\"\n",
    "    print('Assumption 5: Homoscedasticity of Error Terms', '\\n')\n",
    "    \n",
    "    print('Residuals should have relative constant variance')\n",
    "        \n",
    "    # Calculating residuals for the plot\n",
    "    df_results = calculate_residuals(model, features, label)\n",
    "\n",
    "    # Plotting the residuals\n",
    "    plt.subplots(figsize=(12, 6))\n",
    "    ax = plt.subplot(111)  # To remove spines\n",
    "    plt.scatter(x=df_results.index, y=df_results.Residuals, alpha=0.5)\n",
    "    plt.plot(np.repeat(0, df_results.index.max()), color='darkorange', linestyle='--')\n",
    "    ax.spines['right'].set_visible(False)  # Removing the right spine\n",
    "    ax.spines['top'].set_visible(False)  # Removing the top spine\n",
    "    plt.title('Residuals')\n",
    "    plt.show() "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "homoscedasticity_assumption(pipe_linear_regression, X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Linear Regression Assumptions Test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def linear_regression_assumptions(features, label, feature_names=None):\n",
    "    \"\"\"\n",
    "    Tests a linear regression on the model to see if assumptions are being met\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Setting feature names to x1, x2, x3, etc. if they are not defined\n",
    "    if feature_names is None:\n",
    "        feature_names = ['X'+str(feature+1) for feature in range(features.shape[1])]\n",
    "    \n",
    "    print('Fitting linear regression')\n",
    "    # Multi-threading if the dataset is a size where doing so is beneficial\n",
    "    if features.shape[0] < 100000:\n",
    "        model = LinearRegression(n_jobs=-1)\n",
    "    else:\n",
    "        model = LinearRegression()\n",
    "        \n",
    "    model.fit(features, label)\n",
    "    \n",
    "    # Returning linear regression R^2 and coefficients before performing diagnostics\n",
    "    r2 = model.score(features, label)\n",
    "    print()\n",
    "    print('R^2:', r2, '\\n')\n",
    "    print('Coefficients')\n",
    "    print('-------------------------------------')\n",
    "    print('Intercept:', model.intercept_)\n",
    "    \n",
    "    for feature in range(len(model.coef_)):\n",
    "        print('{0}: {1}'.format(feature_names[feature], model.coef_[feature]))\n",
    "\n",
    "    print('\\nPerforming linear regression assumption testing')\n",
    "    \n",
    "    # Creating predictions and calculating residuals for assumption tests\n",
    "    predictions = model.predict(features)\n",
    "    df_results = pd.DataFrame({'Actual': label, 'Predicted': predictions})\n",
    "    df_results['Residuals'] = abs(df_results['Actual']) - abs(df_results['Predicted'])\n",
    "\n",
    "    \n",
    "    def linear_assumption():\n",
    "        \"\"\"\n",
    "        Linearity: Assumes there is a linear relationship between the predictors and\n",
    "                   the response variable. If not, either a polynomial term or another\n",
    "                   algorithm should be used.\n",
    "        \"\"\"\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 1: Linear Relationship between the Target and the Features')\n",
    "        \n",
    "        print('Checking with a scatter plot of actual vs. predicted. Predictions should follow the diagonal line.')\n",
    "        \n",
    "        # Plotting the actual vs predicted values\n",
    "        sns.lmplot(x='Actual', y='Predicted', data=df_results, fit_reg=False, size=7)\n",
    "        \n",
    "        # Plotting the diagonal line\n",
    "        line_coords = np.arange(df_results.min().min(), df_results.max().max())\n",
    "        plt.plot(line_coords, line_coords,  # X and y points\n",
    "                 color='darkorange', linestyle='--')\n",
    "        plt.title('Actual vs. Predicted')\n",
    "        plt.show()\n",
    "        print('If non-linearity is apparent, consider adding a polynomial term')\n",
    "        \n",
    "        \n",
    "    def normal_errors_assumption(p_value_thresh=0.05):\n",
    "        \"\"\"\n",
    "        Normality: Assumes that the error terms are normally distributed. If they are not,\n",
    "        nonlinear transformations of variables may solve this.\n",
    "               \n",
    "        This assumption being violated primarily causes issues with the confidence intervals\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.diagnostic import normal_ad\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 2: The error terms are normally distributed')\n",
    "        print()\n",
    "    \n",
    "        print('Using the Anderson-Darling test for normal distribution')\n",
    "\n",
    "        # Performing the test on the residuals\n",
    "        p_value = normal_ad(df_results['Residuals'])[1]\n",
    "        print('p-value from the test - below 0.05 generally means non-normal:', p_value)\n",
    "    \n",
    "        # Reporting the normality of the residuals\n",
    "        if p_value < p_value_thresh:\n",
    "            print('Residuals are not normally distributed')\n",
    "        else:\n",
    "            print('Residuals are normally distributed')\n",
    "    \n",
    "        # Plotting the residuals distribution\n",
    "        plt.subplots(figsize=(12, 6))\n",
    "        plt.title('Distribution of Residuals')\n",
    "        sns.distplot(df_results['Residuals'])\n",
    "        plt.show()\n",
    "    \n",
    "        print()\n",
    "        if p_value > p_value_thresh:\n",
    "            print('Assumption satisfied')\n",
    "        else:\n",
    "            print('Assumption not satisfied')\n",
    "            print()\n",
    "            print('Confidence intervals will likely be affected')\n",
    "            print('Try performing nonlinear transformations on variables')\n",
    "        \n",
    "        \n",
    "    def multicollinearity_assumption():\n",
    "        \"\"\"\n",
    "        Multicollinearity: Assumes that predictors are not correlated with each other. If there is\n",
    "                           correlation among the predictors, then either remove prepdictors with high\n",
    "                           Variance Inflation Factor (VIF) values or perform dimensionality reduction\n",
    "                           \n",
    "                           This assumption being violated causes issues with interpretability of the \n",
    "                           coefficients and the standard errors of the coefficients.\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 3: Little to no multicollinearity among predictors')\n",
    "        \n",
    "        # Plotting the heatmap\n",
    "        plt.figure(figsize = (10,8))\n",
    "        sns.heatmap(pd.DataFrame(features, columns=feature_names).corr(), annot=True)\n",
    "        plt.title('Correlation of Variables')\n",
    "        plt.show()\n",
    "        \n",
    "        print('Variance Inflation Factors (VIF)')\n",
    "        print('> 10: An indication that multicollinearity may be present')\n",
    "        print('> 100: Certain multicollinearity among the variables')\n",
    "        print('-------------------------------------')\n",
    "       \n",
    "        # Gathering the VIF for each variable\n",
    "        VIF = [variance_inflation_factor(features, i) for i in range(features.shape[1])]\n",
    "        for idx, vif in enumerate(VIF):\n",
    "            print('{0}: {1}'.format(feature_names[idx], vif))\n",
    "        \n",
    "        # Gathering and printing total cases of possible or definite multicollinearity\n",
    "        possible_multicollinearity = sum([1 for vif in VIF if vif > 10])\n",
    "        definite_multicollinearity = sum([1 for vif in VIF if vif > 100])\n",
    "        print()\n",
    "        print('{0} cases of possible multicollinearity'.format(possible_multicollinearity))\n",
    "        print('{0} cases of definite multicollinearity'.format(definite_multicollinearity))\n",
    "        print()\n",
    "\n",
    "        if definite_multicollinearity == 0:\n",
    "            if possible_multicollinearity == 0:\n",
    "                print('Assumption satisfied')\n",
    "            else:\n",
    "                print('Assumption possibly satisfied')\n",
    "                print()\n",
    "                print('Coefficient interpretability may be problematic')\n",
    "                print('Consider removing variables with a high Variance Inflation Factor (VIF)')\n",
    "        else:\n",
    "            print('Assumption not satisfied')\n",
    "            print()\n",
    "            print('Coefficient interpretability will be problematic')\n",
    "            print('Consider removing variables with a high Variance Inflation Factor (VIF)')\n",
    "        \n",
    "        \n",
    "    def autocorrelation_assumption():\n",
    "        \"\"\"\n",
    "        Autocorrelation: Assumes that there is no autocorrelation in the residuals. If there is\n",
    "                         autocorrelation, then there is a pattern that is not explained due to\n",
    "                         the current value being dependent on the previous value.\n",
    "                         This may be resolved by adding a lag variable of either the dependent\n",
    "                         variable or some of the predictors.\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.stattools import durbin_watson\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 4: No Autocorrelation')\n",
    "        print('\\nPerforming Durbin-Watson Test')\n",
    "        print('Values of 1.5 < d < 2.5 generally show that there is no autocorrelation in the data')\n",
    "        print('0 to 2< is positive autocorrelation')\n",
    "        print('>2 to 4 is negative autocorrelation')\n",
    "        print('-------------------------------------')\n",
    "        durbinWatson = durbin_watson(df_results['Residuals'])\n",
    "        print('Durbin-Watson:', durbinWatson)\n",
    "        if durbinWatson < 1.5:\n",
    "            print('Signs of positive autocorrelation', '\\n')\n",
    "            print('Assumption not satisfied', '\\n')\n",
    "            print('Consider adding lag variables')\n",
    "        elif durbinWatson > 2.5:\n",
    "            print('Signs of negative autocorrelation', '\\n')\n",
    "            print('Assumption not satisfied', '\\n')\n",
    "            print('Consider adding lag variables')\n",
    "        else:\n",
    "            print('Little to no autocorrelation', '\\n')\n",
    "            print('Assumption satisfied')\n",
    "\n",
    "            \n",
    "    def homoscedasticity_assumption():\n",
    "        \"\"\"\n",
    "        Homoscedasticity: Assumes that the errors exhibit constant variance\n",
    "        \"\"\"\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 5: Homoscedasticity of Error Terms')\n",
    "        print('Residuals should have relative constant variance')\n",
    "        \n",
    "        # Plotting the residuals\n",
    "        plt.subplots(figsize=(12, 6))\n",
    "        ax = plt.subplot(111)  # To remove spines\n",
    "        plt.scatter(x=df_results.index, y=df_results.Residuals, alpha=0.5)\n",
    "        plt.plot(np.repeat(0, df_results.index.max()), color='darkorange', linestyle='--')\n",
    "        ax.spines['right'].set_visible(False)  # Removing the right spine\n",
    "        ax.spines['top'].set_visible(False)  # Removing the top spine\n",
    "        plt.title('Residuals')\n",
    "        plt.show() \n",
    "        print('If heteroscedasticity is apparent, confidence intervals and predictions will be affected')\n",
    "        \n",
    "        \n",
    "    linear_assumption()\n",
    "    normal_errors_assumption()\n",
    "    multicollinearity_assumption()\n",
    "    autocorrelation_assumption()\n",
    "    homoscedasticity_assumption()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ridge Regression\n",
    "\n",
    "[Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients with l2 regularization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_ridge = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', Ridge())\n",
    "])\n",
    "pipe_ridge.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = pipe_ridge.predict(X_train)\n",
    "y_test_pred = pipe_ridge.predict(X_test)\n",
    "\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2:', metrics.r2_score(y_test, y_test_pred))\n",
    "print('Adjusted R^2:', adjusted_r2_score(X_test, metrics.r2_score(y_test, y_test_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lasso Regression\n",
    "\n",
    "The [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) is a linear model that estimates sparse coefficients with l1 regularization."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_lasso = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', linear_model.Lasso(alpha=0.1))\n",
    "])\n",
    "pipe_lasso.fit(X_train, y_train)\n",
    "y_train_pred = pipe_lasso.predict(X_train)\n",
    "y_test_pred = pipe_lasso.predict(X_test)\n",
    "\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2:', metrics.r2_score(y_test, y_test_pred))\n",
    "print('Adjusted R^2:', adjusted_r2_score(X_test, metrics.r2_score(y_test, y_test_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Elastic-Net\n",
    "\n",
    "[Elastic-Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet) is a linear regression model trained with both l1 and l2 -norm regularization of the coefficients."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_elastic = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', ElasticNet())\n",
    "])\n",
    "pipe_elastic.fit(X_train, y_train)\n",
    "y_train_pred = pipe_elastic.predict(X_train)\n",
    "y_test_pred = pipe_elastic.predict(X_test)\n",
    "\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2:', metrics.r2_score(y_test, y_test_pred))\n",
    "print('Adjusted R^2:', adjusted_r2_score(X_test, metrics.r2_score(y_test, y_test_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest Regressor"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# random forest model\n",
    "params_rf = {'n_estimators': 1000, 'max_depth': 15, 'random_state': 0, 'min_samples_split' : 5, 'n_jobs': -1}\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(**params_rf))\n",
    "])\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "y_train_pred = pipe_rf.predict(X_train)\n",
    "y_test_pred = pipe_rf.predict(X_test)\n",
    "\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2:', metrics.r2_score(y_test, y_test_pred))\n",
    "print('Adjusted R^2:', adjusted_r2_score(X_test, metrics.r2_score(y_test, y_test_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## GradientBoost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# GBM model\n",
    "params_gbm = {'n_estimators': 150, 'max_depth': 5, 'random_state': 0, 'min_samples_leaf' : 10, 'learning_rate': 0.01, 'subsample': 0.7, 'loss': 'ls'}\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_gbm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(**params_gbm))\n",
    "])\n",
    "pipe_gbm.fit(X_train, y_train)\n",
    "y_train_pred = pipe_gbm.predict(X_train)\n",
    "y_test_pred = pipe_gbm.predict(X_test)\n",
    "\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2:', metrics.r2_score(y_test, y_test_pred))\n",
    "print('Adjusted R^2:', adjusted_r2_score(X_test, metrics.r2_score(y_test, y_test_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LightGBM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# LightGBM model\n",
    "params_lightgbm = {'n_estimators': 150, 'max_depth': 5, 'random_state': 0, 'learning_rate': 0.01, 'subsample': 0.7}\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_gbm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LGBMRegressor(**params_lightgbm))\n",
    "])\n",
    "pipe_gbm.fit(X_train, y_train)\n",
    "y_train_pred = pipe_gbm.predict(X_train)\n",
    "y_test_pred = pipe_gbm.predict(X_test)\n",
    "\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2:', metrics.r2_score(y_test, y_test_pred))\n",
    "print('Adjusted R^2:', adjusted_r2_score(X_test, metrics.r2_score(y_test, y_test_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Catboost"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Catboost model\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_catboost = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', CatBoostRegressor(verbose=0, n_estimators=100))\n",
    "])\n",
    "pipe_catboost.fit(X_train, y_train)\n",
    "y_train_pred = pipe_catboost.predict(X_train)\n",
    "y_test_pred = pipe_catboost.predict(X_test)\n",
    "\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2:', metrics.r2_score(y_test, y_test_pred))\n",
    "print('Adjusted R^2:', adjusted_r2_score(X_test, metrics.r2_score(y_test, y_test_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img title=\"GitHub Mark\" src=\"./img/GitHub-Mark-64px.png\" style=\"height: 32px; padding-right: 15px\" alt=\"GitHub Mark\" align=\"left\"> [GitHub repository](https://github.com/pessini/moby-bikes) <br>Author: Leandro Pessini"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c4aaca6d94e52f1ce9cc908cefca5aa099f069e3c4bc9eaa1b64646fc459322"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}