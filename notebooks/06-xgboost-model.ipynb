{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title=\"GitHub Octocat\" src='./img/Octocat.jpg' style='height: 60px; padding-right: 15px' alt=\"Octocat\" align=\"left\"> This notebook is part of a GitHub repository: https://github.com/pessini/moby-bikes \n",
    "<br>MIT Licensed\n",
    "<br>Author: Leandro Pessini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models & Evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# statsmodel\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats as stats\n",
    "\n",
    "# Boost models\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "\n",
    "# Hyperparameter optimization\n",
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "# Custom objects\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "import experiment_tracker as et\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ideas_df = pd.read_excel('../documentation/experiment_tracker.xlsx', sheet_name='Ideas')\n",
    "# experiments_df = pd.read_excel('../documentation/experiment_tracker.xlsx', sheet_name='Experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a new object to keep track of the experiments\n",
    "experiment_tracker = et.ExperimentTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../data/processed/df_train.csv')\n",
    "df_test = pd.read_csv('../data/processed/df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8760, 22)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_train.copy()\n",
    "X = df.drop(['count'], axis=1)\n",
    "y = df.pop('count')\n",
    "all_columns = list(X.columns)\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1464, 22)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = df_test.copy()\n",
    "X_test = test_df.drop(['count'], axis=1)\n",
    "y_test = test_df.pop('count')\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_to_Experiment(dict_scores = None) -> list:\n",
    "    if dict_scores is None:\n",
    "        dict_scores = {}\n",
    "    rsme = et.Score('RSME', '{:.4f}'.format(dict_scores['train_rsme']), '{:.4f}'.format(dict_scores['val_rsme']))\n",
    "    mae = et.Score('MAE', '{:.4f}'.format(dict_scores['train_mae']), '{:.4f}'.format(dict_scores['val_mae']))\n",
    "    return [rsme, mae]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "def preprocessor(predictors: list) -> ColumnTransformer:\n",
    "    # Setting remainder='passthrough' will mean that all columns not specified in the list of “transformers” \n",
    "    #   will be passed through without transformation, instead of being dropped\n",
    "\n",
    "    ##################### Categorical variables #####################\n",
    "    all_cat_vars = ['timesofday','dayofweek','holiday','peak','hour','working_day','season','month']\n",
    "    cat_vars = [categorical_var for categorical_var in all_cat_vars if categorical_var in predictors]\n",
    "\n",
    "    # categorical variables\n",
    "    cat_pipe = Pipeline([\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ])\n",
    "\n",
    "    cat_encoder = 'cat', cat_pipe, cat_vars\n",
    "\n",
    "    ##################### Numerical variables #####################\n",
    "    all_num_vars = ['rain', 'temp', 'rhum','wdsp','temp_r']\n",
    "    num_vars = [numerical_var for numerical_var in all_num_vars if numerical_var in predictors]\n",
    "\n",
    "    num_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler())\n",
    "        # ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "\n",
    "    num_enconder =  'num', num_pipe, num_vars\n",
    "\n",
    "    ##################### Ordinal variables #####################\n",
    "    all_ord_vars = ['wind_speed_group','rainfall_intensity']\n",
    "    ord_vars = [ordinal_var for ordinal_var in all_ord_vars if ordinal_var in predictors]\n",
    "\n",
    "    ordinal_cols_mapping = []\n",
    "    if 'wind_speed_group' in predictors:\n",
    "        ordinal_cols_mapping.append(\n",
    "            {\"col\":\"wind_speed_group\",    \n",
    "            \"mapping\": {\n",
    "                'Calm / Light Breeze': 0, \n",
    "                'Breeze': 1, \n",
    "                'Moderate Breeze': 2, \n",
    "                'Strong Breeze / Near Gale': 3, \n",
    "                'Gale / Storm': 4\n",
    "            }}\n",
    "        )\n",
    "\n",
    "    if 'rainfall_intensity' in predictors:\n",
    "        ordinal_cols_mapping.append(\n",
    "            {\"col\":\"rainfall_intensity\",    \n",
    "            \"mapping\": {\n",
    "                'no rain': 0, \n",
    "                'drizzle': 1, \n",
    "                'light rain': 2, \n",
    "                'moderate rain': 3, \n",
    "                'heavy rain': 4\n",
    "            }}\n",
    "        )\n",
    "\n",
    "    # ordinal variables\n",
    "    ord_pipe = Pipeline([\n",
    "        ('ordinal', ce.OrdinalEncoder(mapping=ordinal_cols_mapping))\n",
    "    ])\n",
    "\n",
    "    ord_enconder =  'ordinal', ord_pipe, ord_vars\n",
    "    \n",
    "    #################################################################################\n",
    "    \n",
    "    orig_vars = [var for var in predictors if var not in cat_vars and var not in num_vars and var not in ord_vars]\n",
    "    orig_enconder = 'pass_vars', 'passthrough', orig_vars\n",
    "     # ['temp_bin','rhum_bin']\n",
    "    # ord_pipe = 'passthrough'\n",
    "\n",
    "    transformers_list = []\n",
    "    transformers_list.append(cat_encoder) if cat_vars else None\n",
    "    transformers_list.append(ord_enconder) if ord_vars else None\n",
    "    transformers_list.append(num_enconder) if num_vars else None\n",
    "    # transformers_list.append(orig_enconder) if orig_vars else None\n",
    "    \n",
    "    return ColumnTransformer(transformers=transformers_list, \n",
    "                             remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_dict(dictionary, function):\n",
    "    return {k: function(v) for k,v in dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold_score(params, predictors, X=X, y=y, n_folds=5, verbose=50, early_stopping_rounds=10):\n",
    "    \n",
    "    pipe_xgboost = Pipeline([\n",
    "        ('preprocessor', preprocessor(predictors)),\n",
    "        ('model', xgb.XGBRegressor(**params))\n",
    "    ])\n",
    "\n",
    "    X = X[[c for c in X.columns if c in predictors]]\n",
    "    cv = KFold(n_splits=n_folds, shuffle=True, random_state=2022)\n",
    "    scores = {\"train_rsme\":[],\"val_rsme\":[],\"train_mae\":[],\"val_mae\":[]}\n",
    "\n",
    "    for n_fold, (train_index, test_index) in enumerate(cv.split(X, y)):\n",
    "        print('#'*40, f'Fold {n_fold+1} out of {cv.n_splits}', '#'*40)\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Xy = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "        X_test_transformed = pipe_xgboost['preprocessor'].fit_transform(X_test)\n",
    "        pipe_xgboost.fit(X_train, y_train,\n",
    "                         model__eval_set=[(X_test_transformed, y_test)], \n",
    "                         model__early_stopping_rounds=early_stopping_rounds,\n",
    "                         model__verbose=verbose)\n",
    "        # pipe_xgboost.fit(X_train, y_train)\n",
    "        # print(pipe_xgboost['model'].evals_result())\n",
    "\n",
    "        # Predict on training and validation set\n",
    "        y_pred_train = pipe_xgboost.predict(X_train)\n",
    "        y_pred_val = pipe_xgboost.predict(X_test)\n",
    "\n",
    "        # Calculate the RSME and MAE\n",
    "        # If squared = True returns MSE value, if False returns RMSE value.\n",
    "        scores['train_rsme'].append(metrics.mean_squared_error(y_train, y_pred_train, squared=False))\n",
    "        scores['val_rsme'].append(metrics.mean_squared_error(y_test, y_pred_val, squared=False))\n",
    "        scores['train_mae'].append(metrics.mean_absolute_error(y_train, y_pred_train))\n",
    "        scores['val_mae'].append(metrics.mean_absolute_error(y_test, y_pred_val))\n",
    "\n",
    "        print(f\"Fold {n_fold+1} - best iteration: {pipe_xgboost['model'].get_booster().best_iteration}\\n\")\n",
    "\n",
    "    return summarize_dict(scores, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictors = ['temp','rhum','dayofweek', 'holiday','timesofday','wdsp','rainfall_intensity','peak','working_day', 'hour', 'season']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning\n",
    "\n",
    "- eta [default=0.3]\n",
    "Analogous to learning rate in GBM\n",
    "Makes the model more robust by shrinking the weights on each step\n",
    "Typical final values to be used: 0.01-0.2\n",
    "- min_child_weight [default=1]\n",
    "Defines the minimum sum of weights of all observations required in a child.\n",
    "This is similar to min_child_leaf in GBM but not exactly. This refers to min “sum of weights” of observations while GBM has min “number of observations”.\n",
    "Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree.\n",
    "Too high values can lead to under-fitting hence, it should be tuned using CV.\n",
    "- max_depth [default=6]\n",
    "The maximum depth of a tree, same as GBM.\n",
    "Used to control over-fitting as higher depth will allow model to learn relations very specific to a particular sample.\n",
    "Should be tuned using CV.\n",
    "Typical values: 3-10\n",
    "- max_leaf_nodes\n",
    "The maximum number of terminal nodes or leaves in a tree.\n",
    "Can be defined in place of max_depth. Since binary trees are created, a depth of ‘n’ would produce a maximum of 2^n leaves.\n",
    "If this is defined, GBM will ignore max_depth.\n",
    "- gamma [default=0]\n",
    "A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.\n",
    "Makes the algorithm conservative. The values can vary depending on the loss function and should be tuned.\n",
    "- max_delta_step [default=0]\n",
    "In maximum delta step we allow each tree’s weight estimation to be. If the value is set to 0, it means there is no constraint. If it is set to a positive value, it can help making the update step more conservative.\n",
    "Usually this parameter is not needed, but it might help in logistic regression when class is extremely imbalanced.\n",
    "This is generally not used but you can explore further if you wish.\n",
    "- subsample [default=1]\n",
    "Same as the subsample of GBM. Denotes the fraction of observations to be randomly samples for each tree.\n",
    "Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.\n",
    "Typical values: 0.5-1\n",
    "- colsample_bytree [default=1]\n",
    "Similar to max_features in GBM. Denotes the fraction of columns to be randomly samples for each tree.\n",
    "Typical values: 0.5-1\n",
    "- colsample_bylevel [default=1]\n",
    "Denotes the subsample ratio of columns for each split, in each level.\n",
    "I don’t use this often because subsample and colsample_bytree will do the job for you. but you can explore further if you feel so.\n",
    "- lambda [default=1]\n",
    "L2 regularization term on weights (analogous to Ridge regression)\n",
    "This used to handle the regularization part of XGBoost. Though many data scientists don’t use it often, it should be explored to reduce overfitting.\n",
    "- alpha [default=0]\n",
    "L1 regularization term on weight (analogous to Lasso regression)\n",
    "Can be used in case of very high dimensionality so that the algorithm runs faster when implemented\n",
    "- scale_pos_weight [default=1]\n",
    "A value greater than 0 should be used in case of high class imbalance as it helps in faster convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## Fold 1 out of 3 ########################################\n",
      "[0]\tvalidation_0-rmse:4.21537\n",
      "[50]\tvalidation_0-rmse:2.74378\n",
      "[60]\tvalidation_0-rmse:2.74738\n",
      "Fold 1 - best iteration: 50\n",
      "\n",
      "######################################## Fold 2 out of 3 ########################################\n",
      "[0]\tvalidation_0-rmse:3.78092\n",
      "[50]\tvalidation_0-rmse:2.50728\n",
      "[52]\tvalidation_0-rmse:2.51101\n",
      "Fold 2 - best iteration: 43\n",
      "\n",
      "######################################## Fold 3 out of 3 ########################################\n",
      "[0]\tvalidation_0-rmse:3.96064\n",
      "[50]\tvalidation_0-rmse:2.65082\n",
      "[60]\tvalidation_0-rmse:2.65222\n",
      "Fold 3 - best iteration: 50\n",
      "\n",
      "--- New Experiment added! ---\n",
      "ID#: 5415556000 \n",
      "Algorithm: XGBoost (Baseline) \n",
      "Predictors: ['temp', 'rhum', 'dayofweek', 'holiday', 'timesofday', 'wdsp', 'rainfall_intensity', 'peak', 'working_day', 'hour', 'season']\n",
      "Hyperparameters: {'max_depth': 3, 'seed': 42, 'eval_metric': 'rmse'}\n",
      "Date: 26/06/2022 11:09:14\n",
      "Metric: [{ 'metric': RSME, 'train': 2.4729,  'validation': 2.6320, 'test': None }, { 'metric': MAE, 'train': 1.8143,  'validation': 1.9269, 'test': None }]\n",
      "Notes: Baseline XGBoost\n"
     ]
    }
   ],
   "source": [
    "# Baseline model\n",
    "predictors = ['temp','rhum','dayofweek', 'holiday','timesofday','wdsp','rainfall_intensity','peak','working_day', 'hour', 'season']\n",
    "\n",
    "params_xgboost = {'max_depth':3,\n",
    "                   'seed': 42,\n",
    "                   'eval_metric': 'rmse'\n",
    "                   }\n",
    "\n",
    "dict_scores = kfold_score(params_xgboost, predictors, n_folds=3)\n",
    "exp_xgboost = et.Experiment('XGBoost (Baseline)', predictors=predictors, hyperparameters=params_xgboost,\n",
    "                               score=get_metrics_to_Experiment(dict_scores), notes='Baseline XGBoost')\n",
    "experiment_tracker.add_experiment(exp_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################## Fold 1 out of 5 ########################################\n",
      "[0]\tvalidation_0-rmse:4.98711\n",
      "[250]\tvalidation_0-rmse:2.74485\n",
      "[376]\tvalidation_0-rmse:2.72610\n",
      "Fold 1 - best iteration: 326\n",
      "\n",
      "######################################## Fold 2 out of 5 ########################################\n",
      "[0]\tvalidation_0-rmse:4.92152\n",
      "[250]\tvalidation_0-rmse:2.70298\n",
      "[500]\tvalidation_0-rmse:2.66915\n",
      "[511]\tvalidation_0-rmse:2.66981\n",
      "Fold 2 - best iteration: 462\n",
      "\n",
      "######################################## Fold 3 out of 5 ########################################\n",
      "[0]\tvalidation_0-rmse:4.68695\n",
      "[250]\tvalidation_0-rmse:2.49219\n",
      "[387]\tvalidation_0-rmse:2.48008\n",
      "Fold 3 - best iteration: 338\n",
      "\n",
      "######################################## Fold 4 out of 5 ########################################\n",
      "[0]\tvalidation_0-rmse:4.71489\n",
      "[250]\tvalidation_0-rmse:2.55270\n",
      "[376]\tvalidation_0-rmse:2.54808\n",
      "Fold 4 - best iteration: 326\n",
      "\n",
      "######################################## Fold 5 out of 5 ########################################\n",
      "[0]\tvalidation_0-rmse:4.85359\n",
      "[250]\tvalidation_0-rmse:2.69569\n",
      "[384]\tvalidation_0-rmse:2.68207\n",
      "Fold 5 - best iteration: 335\n",
      "\n",
      "--- New Experiment added! ---\n",
      "ID#: 5415606208 \n",
      "Algorithm: XGBoost (gamma=0.5) \n",
      "Predictors: ['temp', 'rhum', 'dayofweek', 'holiday', 'timesofday', 'wdsp', 'rainfall_intensity', 'peak', 'working_day', 'hour', 'season']\n",
      "Hyperparameters: {'max_depth': 9, 'eta': 0.01, 'n_estimators': 2000, 'gamma': 1, 'seed': 42, 'eval_metric': 'rmse'}\n",
      "Date: 26/06/2022 11:14:32\n",
      "Metric: [{ 'metric': RSME, 'train': 1.8515,  'validation': 2.6197, 'test': None }, { 'metric': MAE, 'train': 1.3212,  'validation': 1.8837, 'test': None }]\n",
      "Notes: Changed gamma to 0.5\n"
     ]
    }
   ],
   "source": [
    "predictors = ['temp','rhum','dayofweek', 'holiday','timesofday','wdsp','rainfall_intensity','peak','working_day', 'hour', 'season']\n",
    "\n",
    "params_xgboost = {'max_depth':9,\n",
    "                  'eta': 0.01,\n",
    "                  'n_estimators': 2000,\n",
    "                  'gamma': 1,\n",
    "                   'seed': 42,\n",
    "                   'eval_metric': 'rmse'\n",
    "                   }\n",
    "\n",
    "dict_scores = kfold_score(params_xgboost, predictors, n_folds=5, verbose=250, early_stopping_rounds=50)\n",
    "exp_xgboost = et.Experiment('XGBoost (gamma=0.5)', predictors=predictors, hyperparameters=params_xgboost, \n",
    "                            score=get_metrics_to_Experiment(dict_scores), \n",
    "                            notes='Changed gamma to 0.5')\n",
    "experiment_tracker.add_experiment(exp_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment_tracker.to_excel('../documentation/experiment_tracker_xgboost.xlsx')\n",
    "# joblib.dump(pipe_xgboost['model'], '../models/XGBoost.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Leandro Pessini\n",
      "\n",
      "Last updated: Sat Jun 25 2022\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.6\n",
      "IPython version      : 8.3.0\n",
      "\n",
      "seaborn          : 0.11.1\n",
      "category_encoders: 2.4.0\n",
      "statsmodels      : 0.13.2\n",
      "sklearn          : 1.0.2\n",
      "numpy            : 1.21.1\n",
      "pandas           : 1.3.0\n",
      "sys              : 3.9.6 | packaged by conda-forge | (default, Jul 11 2021, 03:36:15) \n",
      "[Clang 11.1.0 ]\n",
      "joblib           : 1.0.1\n",
      "xgboost          : 1.4.0\n",
      "matplotlib       : 3.4.2\n",
      "\n",
      "Watermark: 2.3.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Leandro Pessini\" -n -u -v -iv -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> https://pierpaolo28.github.io/blog/blog25/#bayesian-optimization\n",
    "\n",
    "> https://www.kdnuggets.com/2019/07/xgboost-random-forest-bayesian-optimisation.html\n",
    "\n",
    "> https://www.kaggle.com/code/neerajmohan/randomforest-model-with-bayesian-optimization/notebook\n",
    "\n",
    "> https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title=\"GitHub Mark\" src=\"./img/GitHub-Mark-64px.png\" style=\"height: 32px; padding-right: 15px\" alt=\"GitHub Mark\" align=\"left\"> [GitHub repository](https://github.com/pessini/moby-bikes) <br>Author: Leandro Pessini"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0db2d5a5fe4abf11b42a56673db1407392a211b41c13d1f9324dcbb399c1516f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
