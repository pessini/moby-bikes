{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title=\"GitHub Octocat\" src='./img/Octocat.jpg' style='height: 60px; padding-right: 15px' alt=\"Octocat\" align=\"left\"> This notebook is part of a GitHub repository: https://github.com/pessini/moby-bikes \n",
    "<br>MIT Licensed\n",
    "<br>Author: Leandro Pessini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Model and Evaluation\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/interim/hourly_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/rs/20m5cx314y19rmnyp58yqp5w0000gn/T/ipykernel_48356/1089080596.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhourly_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/interim/hourly_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \"\"\"\n\u001b[0;32m--> 222\u001b[0;31m         self.handles = get_handle(\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/interim/hourly_data.csv'"
     ]
    }
   ],
   "source": [
    "hourly_data = pd.read_csv('../data/interim/hourly_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_numerical_data = hourly_data.filter(['rain','wdsp','rhum'], axis=1)\n",
    "scaled_features = StandardScaler().fit_transform(normalized_numerical_data.values)\n",
    "scaled_features_df = pd.DataFrame(scaled_features, index=normalized_numerical_data.index, columns=normalized_numerical_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features_df.agg(['skew', 'kurtosis']).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data.temp.agg(['skew', 'kurtosis']).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log transformation\n",
    "\n",
    "The logarithmic is a strong transformation that has a major effect on distribution shape. This technique is, as the square root method, oftenly used for reducing right skewness. Worth noting, however, is that it can not be applied to zero or negative values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square Root\n",
    "The square root method is typically used when your data is moderately skewed. Now using the square root (e.g., sqrt(x)) is  a transformation that has a moderate effect on distribution shape. It is generally used to reduce right skewed data. Finally, the square root can be applied on zero values and is most commonly used on counted data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Box Cox Transformation\n",
    "The Box-Cox transformation is, as you probably understand, also a technique to transform non-normal data into normal shape. This is a procedure to identify a suitable exponent (Lambda = l) to use to transform skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(22,6))\n",
    "gs = fig.add_gridspec(1, 3)\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "sns.histplot(scaled_features_df['rain'],ax=ax0, kde=True)\n",
    "sns.histplot(scaled_features_df['wdsp'],ax=ax1, kde=True)\n",
    "sns.histplot(scaled_features_df['rhum'],ax=ax2, kde=True)\n",
    "\n",
    "ax0.set(xlabel='Rain', ylabel='Count',title=\"Distribution - Rain\")\n",
    "ax1.set(xlabel='Wind Speed', ylabel='Count',title=\"Distribution - Wind Speed\")\n",
    "ax2.set(xlabel='Relative Humidity', ylabel='Count',title=\"Distribution - Relative Humidity\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target feature (Count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_data = hourly_data.copy()\n",
    "transform_data['count'] = np.log(transform_data['count'] + 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "gs = fig.add_gridspec(1, 2)\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "sns.histplot(hourly_data['count'],ax=ax0, kde=True)\n",
    "sns.histplot(transform_data['count'],ax=ax1, kde=True)\n",
    "ax0.set(xlabel='Rentals Count',ylabel='Density',title=\"Distribution - Target Feature (Count)\")\n",
    "ax1.set(xlabel='Rentals Count',ylabel='Density',title=\"Distribution of Target variable after Log Transformation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourlyDataWithoutOutliers = hourly_data[np.abs(hourly_data[\"count\"]-hourly_data[\"count\"].mean())<=(2*hourly_data[\"count\"].std())] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 8))\n",
    "sns.boxplot(data=hourlyDataWithoutOutliers,y=\"count\",orient=\"v\",ax=ax)\n",
    "ax.set(ylabel='Count',title=\"Box Plot On Count Withou Outliers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.histplot(hourlyDataWithoutOutliers['count'],ax=ax, kde=True)\n",
    "ax.set(xlabel='Rentals Count',ylabel='Density',title=\"Distribution - Target Feature (Without Outliers)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Shape of original data: \",hourly_data.shape)\n",
    "print (\"Shape of data without outliers: \",hourlyDataWithoutOutliers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Linear Regression Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returning the R^2 for the model\n",
    "linear_r2 = pipe_linear_regression.score(X_train, y_train)\n",
    "print('R^2: {0}'.format(linear_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_residuals(model, features, label):\n",
    "    \"\"\"\n",
    "    Creates predictions on the features with the model and calculates residuals\n",
    "    \"\"\"\n",
    "    predictions = model.predict(features)\n",
    "    df_results = pd.DataFrame({'Actual': label, 'Predicted': predictions})\n",
    "    df_results['Residuals'] = abs(df_results['Actual']) - abs(df_results['Predicted'])\n",
    "    \n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_assumption(model, features, label):\n",
    "    \"\"\"\n",
    "    Linearity: Assumes that there is a linear relationship between the predictors and\n",
    "               the response variable. If not, either a quadratic term or another\n",
    "               algorithm should be used.\n",
    "    \"\"\"\n",
    "    print('Assumption 1: Linear Relationship between the Target and the Feature', '\\n')\n",
    "        \n",
    "    print('Checking with a scatter plot of actual vs. predicted.',\n",
    "           'Predictions should follow the diagonal line.')\n",
    "    \n",
    "    # Calculating residuals for the plot\n",
    "    df_results = calculate_residuals(model, features, label)\n",
    "    \n",
    "    # Plotting the actual vs predicted values\n",
    "    sns.lmplot(x='Actual', y='Predicted', data=df_results, fit_reg=False, height=7)\n",
    "        \n",
    "    # Plotting the diagonal line\n",
    "    line_coords = np.arange(df_results.min().min(), df_results.max().max())\n",
    "    plt.plot(line_coords, line_coords,  # X and y points\n",
    "             color='darkorange', linestyle='--')\n",
    "    plt.title('Actual vs. Predicted')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_assumption(pipe_linear_regression, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normality of the Error Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_errors_assumption(model, features, label, p_value_thresh=0.05):\n",
    "    \"\"\"\n",
    "    Normality: Assumes that the error terms are normally distributed. If they are not,\n",
    "    nonlinear transformations of variables may solve this.\n",
    "               \n",
    "    This assumption being violated primarily causes issues with the confidence intervals\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.diagnostic import normal_ad\n",
    "    print('Assumption 2: The error terms are normally distributed', '\\n')\n",
    "    \n",
    "    # Calculating residuals for the Anderson-Darling test\n",
    "    df_results = calculate_residuals(model, features, label)\n",
    "    \n",
    "    print('Using the Anderson-Darling test for normal distribution')\n",
    "\n",
    "    # Performing the test on the residuals\n",
    "    p_value = normal_ad(df_results['Residuals'])[1]\n",
    "    print('p-value from the test - below 0.05 generally means non-normal:', p_value)\n",
    "    \n",
    "    # Reporting the normality of the residuals\n",
    "    if p_value < p_value_thresh:\n",
    "        print('Residuals are not normally distributed')\n",
    "    else:\n",
    "        print('Residuals are normally distributed')\n",
    "    \n",
    "    # Plotting the residuals distribution\n",
    "    plt.subplots(figsize=(12, 6))\n",
    "    plt.title('Distribution of Residuals')\n",
    "    sns.histplot(df_results['Residuals'], kde=True) \n",
    "    plt.show()\n",
    "    \n",
    "    print()\n",
    "    if p_value > p_value_thresh:\n",
    "        print('Assumption satisfied')\n",
    "    else:\n",
    "        print('Assumption not satisfied')\n",
    "        print()\n",
    "        print('Confidence intervals will likely be affected')\n",
    "        print('Try performing nonlinear transformations on variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_errors_assumption(pipe_linear_regression, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Multicollinearity among Predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multicollinearity_assumption(model, features, label, feature_names=None):\n",
    "    \"\"\"\n",
    "    Multicollinearity: Assumes that predictors are not correlated with each other. If there is\n",
    "                       correlation among the predictors, then either remove prepdictors with high\n",
    "                       Variance Inflation Factor (VIF) values or perform dimensionality reduction\n",
    "                           \n",
    "                       This assumption being violated causes issues with interpretability of the \n",
    "                       coefficients and the standard errors of the coefficients.\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "    print('Assumption 3: Little to no multicollinearity among predictors')\n",
    "        \n",
    "    # Plotting the heatmap\n",
    "    plt.figure(figsize = (10,8))\n",
    "    sns.heatmap(pd.DataFrame(features, columns=feature_names).corr(), annot=True)\n",
    "    plt.title('Correlation of Variables')\n",
    "    plt.show()\n",
    "        \n",
    "    print('Variance Inflation Factors (VIF)')\n",
    "    print('> 10: An indication that multicollinearity may be present')\n",
    "    print('> 100: Certain multicollinearity among the variables')\n",
    "    print('-------------------------------------')\n",
    "       \n",
    "    # Gathering the VIF for each variable\n",
    "    VIF = [variance_inflation_factor(features, i) for i in range(features.shape[1])]\n",
    "    for idx, vif in enumerate(VIF):\n",
    "        print('{0}: {1}'.format(feature_names[idx], vif))\n",
    "        \n",
    "    # Gathering and printing total cases of possible or definite multicollinearity\n",
    "    possible_multicollinearity = sum([1 for vif in VIF if vif > 10])\n",
    "    definite_multicollinearity = sum([1 for vif in VIF if vif > 100])\n",
    "    print()\n",
    "    print('{0} cases of possible multicollinearity'.format(possible_multicollinearity))\n",
    "    print('{0} cases of definite multicollinearity'.format(definite_multicollinearity))\n",
    "    print()\n",
    "\n",
    "    if definite_multicollinearity == 0:\n",
    "        if possible_multicollinearity == 0:\n",
    "            print('Assumption satisfied')\n",
    "        else:\n",
    "            print('Assumption possibly satisfied')\n",
    "            print()\n",
    "            print('Coefficient interpretability may be problematic')\n",
    "            print('Consider removing variables with a high Variance Inflation Factor (VIF)')\n",
    "\n",
    "    else:\n",
    "        print('Assumption not satisfied')\n",
    "        print()\n",
    "        print('Coefficient interpretability will be problematic')\n",
    "        print('Consider removing variables with a high Variance Inflation Factor (VIF)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multicollinearity_assumption(pipe_linear_regression, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Autocorrelation of the Error Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation_assumption(model, features, label):\n",
    "    \"\"\"\n",
    "    Autocorrelation: Assumes that there is no autocorrelation in the residuals. If there is\n",
    "                     autocorrelation, then there is a pattern that is not explained due to\n",
    "                     the current value being dependent on the previous value.\n",
    "                     This may be resolved by adding a lag variable of either the dependent\n",
    "                     variable or some of the predictors.\n",
    "    \"\"\"\n",
    "    from statsmodels.stats.stattools import durbin_watson\n",
    "    print('Assumption 4: No Autocorrelation', '\\n')\n",
    "    \n",
    "    # Calculating residuals for the Durbin Watson-tests\n",
    "    df_results = calculate_residuals(model, features, label)\n",
    "\n",
    "    print('\\nPerforming Durbin-Watson Test')\n",
    "    print('Values of 1.5 < d < 2.5 generally show that there is no autocorrelation in the data')\n",
    "    print('0 to 2< is positive autocorrelation')\n",
    "    print('>2 to 4 is negative autocorrelation')\n",
    "    print('-------------------------------------')\n",
    "    durbinWatson = durbin_watson(df_results['Residuals'])\n",
    "    print('Durbin-Watson:', durbinWatson)\n",
    "    if durbinWatson < 1.5:\n",
    "        print('Signs of positive autocorrelation', '\\n')\n",
    "        print('Assumption not satisfied')\n",
    "    elif durbinWatson > 2.5:\n",
    "        print('Signs of negative autocorrelation', '\\n')\n",
    "        print('Assumption not satisfied')\n",
    "    else:\n",
    "        print('Little to no autocorrelation', '\\n')\n",
    "        print('Assumption satisfied')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autocorrelation_assumption(pipe_linear_regression, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def homoscedasticity_assumption(model, features, label):\n",
    "    \"\"\"\n",
    "    Homoscedasticity: Assumes that the errors exhibit constant variance\n",
    "    \"\"\"\n",
    "    print('Assumption 5: Homoscedasticity of Error Terms', '\\n')\n",
    "    \n",
    "    print('Residuals should have relative constant variance')\n",
    "        \n",
    "    # Calculating residuals for the plot\n",
    "    df_results = calculate_residuals(model, features, label)\n",
    "\n",
    "    # Plotting the residuals\n",
    "    plt.subplots(figsize=(12, 6))\n",
    "    ax = plt.subplot(111)  # To remove spines\n",
    "    plt.scatter(x=df_results.index, y=df_results.Residuals, alpha=0.5)\n",
    "    plt.plot(np.repeat(0, df_results.index.max()), color='darkorange', linestyle='--')\n",
    "    ax.spines['right'].set_visible(False)  # Removing the right spine\n",
    "    ax.spines['top'].set_visible(False)  # Removing the top spine\n",
    "    plt.title('Residuals')\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homoscedasticity_assumption(pipe_linear_regression, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Assumptions Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Assumptions\n",
    "\n",
    "- Linearity\n",
    "- Normality of the Error\n",
    "- No Multicollinearity among Predictors\n",
    "- No Autocorrelation of the Error Terms\n",
    "- Homoscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_assumptions(features, label, feature_names=None):\n",
    "    \"\"\"\n",
    "    Tests a linear regression on the model to see if assumptions are being met\n",
    "    \"\"\"\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    \n",
    "    # Setting feature names to x1, x2, x3, etc. if they are not defined\n",
    "    if feature_names is None:\n",
    "        feature_names = ['X'+str(feature+1) for feature in range(features.shape[1])]\n",
    "    \n",
    "    print('Fitting linear regression')\n",
    "    # Multi-threading if the dataset is a size where doing so is beneficial\n",
    "    if features.shape[0] < 100000:\n",
    "        model = LinearRegression(n_jobs=-1)\n",
    "    else:\n",
    "        model = LinearRegression()\n",
    "        \n",
    "    model.fit(features, label)\n",
    "    \n",
    "    # Returning linear regression R^2 and coefficients before performing diagnostics\n",
    "    r2 = model.score(features, label)\n",
    "    print()\n",
    "    print('R^2:', r2, '\\n')\n",
    "    print('Coefficients')\n",
    "    print('-------------------------------------')\n",
    "    print('Intercept:', model.intercept_)\n",
    "    \n",
    "    for feature in range(len(model.coef_)):\n",
    "        print('{0}: {1}'.format(feature_names[feature], model.coef_[feature]))\n",
    "\n",
    "    print('\\nPerforming linear regression assumption testing')\n",
    "    \n",
    "    # Creating predictions and calculating residuals for assumption tests\n",
    "    predictions = model.predict(features)\n",
    "    df_results = pd.DataFrame({'Actual': label, 'Predicted': predictions})\n",
    "    df_results['Residuals'] = abs(df_results['Actual']) - abs(df_results['Predicted'])\n",
    "\n",
    "    \n",
    "    def linear_assumption():\n",
    "        \"\"\"\n",
    "        Linearity: Assumes there is a linear relationship between the predictors and\n",
    "                   the response variable. If not, either a polynomial term or another\n",
    "                   algorithm should be used.\n",
    "        \"\"\"\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 1: Linear Relationship between the Target and the Features')\n",
    "        \n",
    "        print('Checking with a scatter plot of actual vs. predicted. Predictions should follow the diagonal line.')\n",
    "        \n",
    "        # Plotting the actual vs predicted values\n",
    "        sns.lmplot(x='Actual', y='Predicted', data=df_results, fit_reg=False, size=7)\n",
    "        \n",
    "        # Plotting the diagonal line\n",
    "        line_coords = np.arange(df_results.min().min(), df_results.max().max())\n",
    "        plt.plot(line_coords, line_coords,  # X and y points\n",
    "                 color='darkorange', linestyle='--')\n",
    "        plt.title('Actual vs. Predicted')\n",
    "        plt.show()\n",
    "        print('If non-linearity is apparent, consider adding a polynomial term')\n",
    "        \n",
    "        \n",
    "    def normal_errors_assumption(p_value_thresh=0.05):\n",
    "        \"\"\"\n",
    "        Normality: Assumes that the error terms are normally distributed. If they are not,\n",
    "        nonlinear transformations of variables may solve this.\n",
    "               \n",
    "        This assumption being violated primarily causes issues with the confidence intervals\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.diagnostic import normal_ad\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 2: The error terms are normally distributed')\n",
    "        print()\n",
    "    \n",
    "        print('Using the Anderson-Darling test for normal distribution')\n",
    "\n",
    "        # Performing the test on the residuals\n",
    "        p_value = normal_ad(df_results['Residuals'])[1]\n",
    "        print('p-value from the test - below 0.05 generally means non-normal:', p_value)\n",
    "    \n",
    "        # Reporting the normality of the residuals\n",
    "        if p_value < p_value_thresh:\n",
    "            print('Residuals are not normally distributed')\n",
    "        else:\n",
    "            print('Residuals are normally distributed')\n",
    "    \n",
    "        # Plotting the residuals distribution\n",
    "        plt.subplots(figsize=(12, 6))\n",
    "        plt.title('Distribution of Residuals')\n",
    "        sns.distplot(df_results['Residuals'])\n",
    "        plt.show()\n",
    "    \n",
    "        print()\n",
    "        if p_value > p_value_thresh:\n",
    "            print('Assumption satisfied')\n",
    "        else:\n",
    "            print('Assumption not satisfied')\n",
    "            print()\n",
    "            print('Confidence intervals will likely be affected')\n",
    "            print('Try performing nonlinear transformations on variables')\n",
    "        \n",
    "        \n",
    "    def multicollinearity_assumption():\n",
    "        \"\"\"\n",
    "        Multicollinearity: Assumes that predictors are not correlated with each other. If there is\n",
    "                           correlation among the predictors, then either remove prepdictors with high\n",
    "                           Variance Inflation Factor (VIF) values or perform dimensionality reduction\n",
    "                           \n",
    "                           This assumption being violated causes issues with interpretability of the \n",
    "                           coefficients and the standard errors of the coefficients.\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 3: Little to no multicollinearity among predictors')\n",
    "        \n",
    "        # Plotting the heatmap\n",
    "        plt.figure(figsize = (10,8))\n",
    "        sns.heatmap(pd.DataFrame(features, columns=feature_names).corr(), annot=True)\n",
    "        plt.title('Correlation of Variables')\n",
    "        plt.show()\n",
    "        \n",
    "        print('Variance Inflation Factors (VIF)')\n",
    "        print('> 10: An indication that multicollinearity may be present')\n",
    "        print('> 100: Certain multicollinearity among the variables')\n",
    "        print('-------------------------------------')\n",
    "       \n",
    "        # Gathering the VIF for each variable\n",
    "        VIF = [variance_inflation_factor(features, i) for i in range(features.shape[1])]\n",
    "        for idx, vif in enumerate(VIF):\n",
    "            print('{0}: {1}'.format(feature_names[idx], vif))\n",
    "        \n",
    "        # Gathering and printing total cases of possible or definite multicollinearity\n",
    "        possible_multicollinearity = sum([1 for vif in VIF if vif > 10])\n",
    "        definite_multicollinearity = sum([1 for vif in VIF if vif > 100])\n",
    "        print()\n",
    "        print('{0} cases of possible multicollinearity'.format(possible_multicollinearity))\n",
    "        print('{0} cases of definite multicollinearity'.format(definite_multicollinearity))\n",
    "        print()\n",
    "\n",
    "        if definite_multicollinearity == 0:\n",
    "            if possible_multicollinearity == 0:\n",
    "                print('Assumption satisfied')\n",
    "            else:\n",
    "                print('Assumption possibly satisfied')\n",
    "                print()\n",
    "                print('Coefficient interpretability may be problematic')\n",
    "                print('Consider removing variables with a high Variance Inflation Factor (VIF)')\n",
    "        else:\n",
    "            print('Assumption not satisfied')\n",
    "            print()\n",
    "            print('Coefficient interpretability will be problematic')\n",
    "            print('Consider removing variables with a high Variance Inflation Factor (VIF)')\n",
    "        \n",
    "        \n",
    "    def autocorrelation_assumption():\n",
    "        \"\"\"\n",
    "        Autocorrelation: Assumes that there is no autocorrelation in the residuals. If there is\n",
    "                         autocorrelation, then there is a pattern that is not explained due to\n",
    "                         the current value being dependent on the previous value.\n",
    "                         This may be resolved by adding a lag variable of either the dependent\n",
    "                         variable or some of the predictors.\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.stattools import durbin_watson\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 4: No Autocorrelation')\n",
    "        print('\\nPerforming Durbin-Watson Test')\n",
    "        print('Values of 1.5 < d < 2.5 generally show that there is no autocorrelation in the data')\n",
    "        print('0 to 2< is positive autocorrelation')\n",
    "        print('>2 to 4 is negative autocorrelation')\n",
    "        print('-------------------------------------')\n",
    "        durbinWatson = durbin_watson(df_results['Residuals'])\n",
    "        print('Durbin-Watson:', durbinWatson)\n",
    "        if durbinWatson < 1.5:\n",
    "            print('Signs of positive autocorrelation', '\\n')\n",
    "            print('Assumption not satisfied', '\\n')\n",
    "            print('Consider adding lag variables')\n",
    "        elif durbinWatson > 2.5:\n",
    "            print('Signs of negative autocorrelation', '\\n')\n",
    "            print('Assumption not satisfied', '\\n')\n",
    "            print('Consider adding lag variables')\n",
    "        else:\n",
    "            print('Little to no autocorrelation', '\\n')\n",
    "            print('Assumption satisfied')\n",
    "\n",
    "            \n",
    "    def homoscedasticity_assumption():\n",
    "        \"\"\"\n",
    "        Homoscedasticity: Assumes that the errors exhibit constant variance\n",
    "        \"\"\"\n",
    "        print('\\n=======================================================================================')\n",
    "        print('Assumption 5: Homoscedasticity of Error Terms')\n",
    "        print('Residuals should have relative constant variance')\n",
    "        \n",
    "        # Plotting the residuals\n",
    "        plt.subplots(figsize=(12, 6))\n",
    "        ax = plt.subplot(111)  # To remove spines\n",
    "        plt.scatter(x=df_results.index, y=df_results.Residuals, alpha=0.5)\n",
    "        plt.plot(np.repeat(0, df_results.index.max()), color='darkorange', linestyle='--')\n",
    "        ax.spines['right'].set_visible(False)  # Removing the right spine\n",
    "        ax.spines['top'].set_visible(False)  # Removing the top spine\n",
    "        plt.title('Residuals')\n",
    "        plt.show() \n",
    "        print('If heteroscedasticity is apparent, confidence intervals and predictions will be affected')\n",
    "        \n",
    "        \n",
    "    linear_assumption()\n",
    "    normal_errors_assumption()\n",
    "    multicollinearity_assumption()\n",
    "    autocorrelation_assumption()\n",
    "    homoscedasticity_assumption()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c82cf216bcd6695c751af4e033b89e10a78cc5d50e2943f0ed5dd08b475eddb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
