{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title=\"GitHub Octocat\" src='./img/Octocat.jpg' style='height: 60px; padding-right: 15px' alt=\"Octocat\" align=\"left\"> This notebook is part of a GitHub repository: https://github.com/pessini/moby-bikes \n",
    "<br>MIT Licensed\n",
    "<br>Author: Leandro Pessini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"font-size:100%; text-align:left; color:#444444;\">Models</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"font-size:100%; text-align:left; color:#444444;\">Table of Contents:</p>\n",
    "* [1. Datasets](#1)\n",
    "  * [1.1 Rentals Data - Moby Bikes](#1.1)\n",
    "  * [1.2 Weather Data - Met Éireann](#1.2)\n",
    "* [2. Preprocessing & Feature Engineering](#2)\n",
    "  * [2.1 Target variable distribution](#2.1)\n",
    "  * [2.2 Missing values](#2.2)\n",
    "  * [2.3 Exploratory Analysis](#2.3)\n",
    "  * [2.4 Features Importance](#2.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models & Evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# statsmodel\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.stats as stats\n",
    "\n",
    "# Boost models\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import catboost as cat\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# Hyperparameter optimization\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# Custom objects\n",
    "sys.path.insert(0, os.path.abspath('../src/'))\n",
    "import experiment_tracker as et\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', FutureWarning)\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8760 entries, 0 to 8759\n",
      "Data columns (total 23 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   rain                8760 non-null   float64\n",
      " 1   temp                8760 non-null   float64\n",
      " 2   rhum                8760 non-null   int64  \n",
      " 3   wdsp                8760 non-null   int64  \n",
      " 4   date                8760 non-null   object \n",
      " 5   hour                8760 non-null   int64  \n",
      " 6   day                 8760 non-null   int64  \n",
      " 7   month               8760 non-null   int64  \n",
      " 8   year                8760 non-null   int64  \n",
      " 9   count               8760 non-null   int64  \n",
      " 10  holiday             8760 non-null   bool   \n",
      " 11  dayofweek_n         8760 non-null   int64  \n",
      " 12  dayofweek           8760 non-null   object \n",
      " 13  working_day         8760 non-null   bool   \n",
      " 14  season              8760 non-null   object \n",
      " 15  peak                8760 non-null   bool   \n",
      " 16  timesofday          8760 non-null   object \n",
      " 17  rainfall_intensity  8760 non-null   object \n",
      " 18  wind_bft            8760 non-null   int64  \n",
      " 19  wind_speed_group    8760 non-null   object \n",
      " 20  temp_r              8760 non-null   int64  \n",
      " 21  temp_bin            8760 non-null   float64\n",
      " 22  rhum_bin            8760 non-null   float64\n",
      "dtypes: bool(3), float64(4), int64(10), object(6)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv('../data/processed/df_train.csv')\n",
    "df_test = pd.read_csv('../data/processed/df_test.csv')\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Example of dynamic table with evalution metrics: https://www.kirenz.com/post/2021-12-06-regression-splines-in-python/regression-splines-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data in train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OrdinalEnconder\n",
    "# enc_rain = OrdinalEncoder(dtype=np.int64, \\\n",
    "#     categories=[['no rain', 'drizzle', 'light rain', 'moderate rain', 'heavy rain']])\n",
    "# df['rainfall_intensity'] = enc_rain.fit_transform(df[['rainfall_intensity']])\n",
    "\n",
    "# num_vars = [n for n in df.select_dtypes(include=['number']).columns if n in predictors] # list comprehension to select only predictors features\n",
    "# cat_vars = [c for c in df.select_dtypes(include=['category']).columns if c in predictors]\n",
    "\n",
    "# dummies = pd.get_dummies(X[cat_vars], drop_first=False)\n",
    "# X = pd.concat([X[num_vars], dummies],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a new object to keep track of the experiments\n",
    "experiment_tracker = et.ExperimentTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6132, 22), (2628, 22))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_train.copy()\n",
    "X = df.drop(['count'], axis=1)\n",
    "y = df.pop('count')\n",
    "all_columns = list(X.columns)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New Idea added! ---\n",
      "ID#: 4597113952 \n",
      "Idea: Dummy Regressor \n",
      "Potential Outcome: To use as a baseline model, expected to be perform bad\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idea_dummy = et.Idea(idea='Dummy Regressor', potential_outcome='To use as a baseline model, expected to be perform bad')\n",
    "experiment_tracker.new_idea(idea_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_score(model, predictors, X_train=X_train, y_train=y_train, X_val=X_val, y_val=y_val):\n",
    "    X_train = X_train[[c for c in X_train.columns if c in predictors]]\n",
    "    X_val = X_val[[c for c in X_val.columns if c in predictors]]\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    train_rsme = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "    val_rsme = metrics.mean_squared_error(y_val, y_pred_val)\n",
    "    train_mae = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "    val_mae = metrics.mean_absolute_error(y_val, y_pred_val)\n",
    "    return train_rsme, val_rsme, train_mae, val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_to_Experiment() -> list:\n",
    "    rsme = et.Score('RSME', '{:.2f}'.format(train_rsme), '{:.2f}'.format(val_rsme))\n",
    "    mae = et.Score('MAE', '{:.2f}'.format(train_mae), '{:.2f}'.format(val_mae))\n",
    "    return [rsme, mae]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "predictors = ['temp','rhum','wdsp','rain']\n",
    "dummy_regr = DummyRegressor(strategy=\"mean\")\n",
    "train_rsme, val_rsme, train_mae, val_mae = get_train_val_score(dummy_regr, predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New Experiment added! ---\n",
      "ID#: 5603562736 \n",
      "Algorithm: Dummy Regressor \n",
      "Predictors: ['temp', 'rhum', 'wdsp', 'rain']\n",
      "Hyperparameters: {'constant': None, 'quantile': None, 'strategy': 'mean'}\n",
      "Date: 28/05/2022 16:00:35\n",
      "Metric: [{ 'metric': RSME, 'train': 13.06,  'validation': 12.85, 'test': None }, { 'metric': MAE, 'train': 2.88,  'validation': 2.90, 'test': None }]\n",
      "Notes: Baseline Model for comparison\n"
     ]
    }
   ],
   "source": [
    "# squared => If True returns MSE value, if False returns RMSE value\n",
    "exp_dummy_regr = et.Experiment('Dummy Regressor', predictors=predictors, hyperparameters=dummy_regr.get_params(), \n",
    "                               score=get_metrics_to_Experiment(), notes='Baseline Model for comparison')\n",
    "experiment_tracker.add_experiment(exp_dummy_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New Idea added! ---\n",
      "ID#: 5603561872 \n",
      "Idea: Linear Regression \n",
      "Potential Outcome: Expected to perform bad as we have many outliers and a count as target variable\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idea_linear = et.Idea(idea='Linear Regression', potential_outcome='Expected to perform bad as we have many outliers and a count as target variable')\n",
    "experiment_tracker.new_idea(idea_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New Experiment added! ---\n",
      "ID#: 4597113856 \n",
      "Algorithm: Linear Regression \n",
      "Predictors: ['temp', 'rhum', 'wdsp', 'rain']\n",
      "Hyperparameters: \n",
      "Date: 28/05/2022 16:00:36\n",
      "Metric: [{ 'metric': RSME, 'train': 9.54,  'validation': 9.57, 'test': None }, { 'metric': MAE, 'train': 2.37,  'validation': 2.40, 'test': None }]\n",
      "Notes: Linear Regression\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "predictors = ['temp','rhum','wdsp','rain']\n",
    "lin_reg = LinearRegression()\n",
    "train_rsme, val_rsme, train_mae, val_mae = get_train_val_score(lin_reg, predictors)\n",
    "exp_lin_regr = et.Experiment('Linear Regression', predictors=predictors, hyperparameters='', \n",
    "                               score=get_metrics_to_Experiment(), notes='Linear Regression')\n",
    "experiment_tracker.add_experiment(exp_lin_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Idea updated! ---\n",
      "ID#: 5603561872 \n",
      "Idea: Linear Regression \n",
      "Potential Outcome: Expected to perform bad as we have many outliers and a count as target variable\n",
      "Learnings: As expected the performance is not good as we have a lot of outliers and a count as target variable\n"
     ]
    }
   ],
   "source": [
    "experiment_tracker.update_idea(idea_linear, learnings='As expected the performance is not good as we have a lot of outliers and a count as target variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New Idea added! ---\n",
      "ID#: 4597435840 \n",
      "Idea: Random Forest \n",
      "Potential Outcome: Expected to perform better than the Linear Regression\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idea_random_forest = et.Idea(idea='Random Forest', potential_outcome='Expected to perform better than the Linear Regression')\n",
    "experiment_tracker.new_idea(idea_random_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New Experiment added! ---\n",
      "ID#: 5603565280 \n",
      "Algorithm: Random Forest \n",
      "Predictors: ['temp', 'rhum', 'wdsp', 'rain']\n",
      "Hyperparameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 10, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "Date: 28/05/2022 16:00:37\n",
      "Metric: [{ 'metric': RSME, 'train': 6.44,  'validation': 9.48, 'test': None }, { 'metric': MAE, 'train': 1.97,  'validation': 2.39, 'test': None }]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "predictors = ['temp','rhum','wdsp','rain']\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "train_rsme, val_rsme, train_mae, val_mae = get_train_val_score(rf, predictors)\n",
    "exp_rf_regr = et.Experiment('Random Forest', predictors=predictors, hyperparameters=rf.get_params(), \n",
    "                               score=get_metrics_to_Experiment(), notes='')\n",
    "experiment_tracker.add_experiment(exp_rf_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Idea updated! ---\n",
      "ID#: 4597435840 \n",
      "Idea: Random Forest \n",
      "Potential Outcome: Expected to perform better than the Linear Regression\n",
      "Learnings: Random Forest with just a few hyperparameters performed a lot better than linear regression.\n",
      "It seems to be overfitting as we see validation scores much higher than training scores. It's a sign that would need to tune hyperparameters.\n"
     ]
    }
   ],
   "source": [
    "# update ideas with learnings\n",
    "learnings = \\\n",
    "\"\"\"Random Forest with just a few hyperparameters performed a lot better than linear regression.\n",
    "It seems to be overfitting as we see validation scores much higher than training scores. It's a sign that would need to tune hyperparameters.\"\"\"\n",
    "experiment_tracker.update_idea(idea_random_forest, learnings=str.strip(learnings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New Experiment added! ---\n",
      "ID#: 4607307440 \n",
      "Algorithm: Random Forest \n",
      "Predictors: ['temp', 'rhum', 'wdsp', 'rain', 'holiday']\n",
      "Hyperparameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 10, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "Date: 28/05/2022 16:00:38\n",
      "Metric: [{ 'metric': RSME, 'train': 6.47,  'validation': 9.46, 'test': None }, { 'metric': MAE, 'train': 1.97,  'validation': 2.38, 'test': None }]\n",
      "Notes: Added holiday as a predictor\n"
     ]
    }
   ],
   "source": [
    "predictors = ['temp','rhum','wdsp','rain','holiday']\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "train_rsme, val_rsme, train_mae, val_mae = get_train_val_score(rf, predictors)\n",
    "exp_rf_regr = et.Experiment('Random Forest', predictors=predictors, hyperparameters=rf.get_params(), \n",
    "                               score=get_metrics_to_Experiment(), notes='Added holiday as a predictor')\n",
    "experiment_tracker.add_experiment(exp_rf_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- New Experiment added! ---\n",
      "ID#: 4597427456 \n",
      "Algorithm: Random Forest \n",
      "Predictors: ['temp', 'rhum', 'wdsp', 'rain', 'holiday', 'dayofweek_n']\n",
      "Hyperparameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 10, 'max_features': 'auto', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "Date: 28/05/2022 16:00:40\n",
      "Metric: [{ 'metric': RSME, 'train': 5.98,  'validation': 9.21, 'test': None }, { 'metric': MAE, 'train': 1.90,  'validation': 2.35, 'test': None }]\n",
      "Notes: Added holiday + dayofweek as a predictor\n"
     ]
    }
   ],
   "source": [
    "predictors = ['temp','rhum','wdsp','rain','holiday','dayofweek_n']\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "train_rsme, val_rsme, train_mae, val_mae = get_train_val_score(rf, predictors)\n",
    "exp_rf_regr = et.Experiment('Random Forest', predictors=predictors, hyperparameters=rf.get_params(), \n",
    "                               score=get_metrics_to_Experiment(), notes='Added holiday + dayofweek as a predictor')\n",
    "experiment_tracker.add_experiment(exp_rf_regr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">https://www.delftstack.com/howto/python-pandas/split-column-in-python-pandas/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ord_vars = ['wind_speed_group','rainfall_intensity','temp_bin','rhum_bin']\n",
    "\n",
    "num_vars = ['rain', 'temp', 'rhum','wdsp','temp_r']\n",
    "\n",
    "cat_vars = ['timesofday','dayofweek','holiday','peak','hour','working_day','season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "ordinal_cols_mapping = [\n",
    "    {\"col\":\"rainfall_intensity\",    \n",
    "    \"mapping\": {\n",
    "        'no rain': 0, \n",
    "        'drizzle': 1, \n",
    "        'light rain': 2, \n",
    "        'moderate rain': 3, \n",
    "        'heavy rain': 4\n",
    "    }},\n",
    "    {\"col\":\"wind_speed_group\",    \n",
    "    \"mapping\": {\n",
    "        'Calm / Light Breeze': 0, \n",
    "        'Breeze': 1, \n",
    "        'Moderate Breeze': 2, \n",
    "        'Strong Breeze / Near Gale': 3, \n",
    "        'Gale / Storm': 4\n",
    "    }}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor_cat_enconder(predictors: list):\n",
    "    cat_vars = ['timesofday','dayofweek','holiday','peak','hour','working_day','season']\n",
    "    \n",
    "    if all(predictors != var for var in cat_vars):\n",
    "        return None\n",
    "    \n",
    "    # categorical variables\n",
    "    cat_pipe = Pipeline([\n",
    "        ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "    ])\n",
    "        \n",
    "    return 'cat', cat_pipe, cat_vars\n",
    "\n",
    "def preprocessor_num_enconder(predictors: list):\n",
    "    num_vars = ['rain', 'temp', 'rhum','wdsp','temp_r']\n",
    "    if all(predictors != var for var in num_vars):\n",
    "        return None\n",
    "    \n",
    "     # numerical variables\n",
    "    num_pipe = Pipeline([\n",
    "        ('scaler', StandardScaler())\n",
    "        # ('scaler', MinMaxScaler())\n",
    "    ])\n",
    "    \n",
    "    return 'num', num_pipe, num_vars\n",
    "    \n",
    "def preprocessor_ordinal_enconder(predictors: list):\n",
    "    ord_vars = ['wind_speed_group','rainfall_intensity']\n",
    "    # ord_vars = ['wind_speed_group','rainfall_intensity','temp_bin','rhum_bin']\n",
    "    if all(predictors != var for var in ord_vars):\n",
    "        return None\n",
    "    \n",
    "    # ordinal variables\n",
    "    ord_pipe = Pipeline([\n",
    "        ('ordinal_enconder', ce.OrdinalEncoder(mapping=ordinal_cols_mapping))\n",
    "    ])\n",
    "\n",
    "    return 'ordinal_enconder', ord_pipe, ord_vars\n",
    "\n",
    "def preprocessor(predictors: list) -> ColumnTransformer:\n",
    "    \n",
    "    # predictors = wind_speed_group, rainfall_intensity, rain, temp\n",
    "    # Setting remainder='passthrough' will mean that all columns not specified in the list of “transformers” \n",
    "    #   will be passed through without transformation, instead of being dropped\n",
    "    \n",
    "    return ColumnTransformer([preprocessor_cat_enconder(predictors), \n",
    "                              preprocessor_ordinal_enconder(predictors), \n",
    "                              preprocessor_num_enconder(predictors)], \n",
    "                             remainder='passthrough')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(model, columns, plot_title='Feature Importances'):\n",
    "    #Plotting features importance\n",
    "    feature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,columns)), \n",
    "                            columns=['Value','Feature'])\n",
    "    scaler_ft = MinMaxScaler()\n",
    "    feature_imp['Value'] = scaler_ft.fit_transform(feature_imp['Value'].values.reshape(-1,1));\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 12))\n",
    "    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "    plt.title(plot_title)\n",
    "    locs, labels = plt.xticks()\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb Cell 35'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000085?line=9'>10</a>\u001b[0m pipe_rf \u001b[39m=\u001b[39m Pipeline([\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000085?line=10'>11</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mpreprocessor\u001b[39m\u001b[39m'\u001b[39m, preprocessor(predictors)),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000085?line=11'>12</a>\u001b[0m     (\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m, RandomForestRegressor(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams_rf, criterion\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmae\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000085?line=12'>13</a>\u001b[0m ])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000085?line=13'>14</a>\u001b[0m \u001b[39m# pipe_rf.fit(X_train[predictors], y_train)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000085?line=14'>15</a>\u001b[0m train_rsme, val_rsme, train_mae, val_mae \u001b[39m=\u001b[39m get_train_val_score(pipe_rf, predictors)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000085?line=15'>16</a>\u001b[0m exp_rf_regr \u001b[39m=\u001b[39m et\u001b[39m.\u001b[39mExperiment(\u001b[39m'\u001b[39m\u001b[39mRandom Forest\u001b[39m\u001b[39m'\u001b[39m, predictors\u001b[39m=\u001b[39mpredictors, hyperparameters\u001b[39m=\u001b[39mpipe_rf[\u001b[39m'\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mget_params(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000085?line=16'>17</a>\u001b[0m                                score\u001b[39m=\u001b[39mget_metrics_to_Experiment(), notes\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAdded all predictors and using preprocessing\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000085?line=17'>18</a>\u001b[0m experiment_tracker\u001b[39m.\u001b[39madd_experiment(exp_rf_regr)\n",
      "\u001b[1;32m/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb Cell 12'\u001b[0m in \u001b[0;36mget_train_val_score\u001b[0;34m(model, predictors, X_train, y_train, X_val, y_val)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000055?line=1'>2</a>\u001b[0m X_train \u001b[39m=\u001b[39m X_train[[c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m X_train\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m c \u001b[39min\u001b[39;00m predictors]]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000055?line=2'>3</a>\u001b[0m X_val \u001b[39m=\u001b[39m X_val[[c \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m X_val\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m c \u001b[39min\u001b[39;00m predictors]]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000055?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000055?line=4'>5</a>\u001b[0m y_pred_train \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/05-modeling.ipynb#ch0000055?line=5'>6</a>\u001b[0m y_pred_val \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m~/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py:390\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=363'>364</a>\u001b[0m \u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=364'>365</a>\u001b[0m \n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=365'>366</a>\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=386'>387</a>\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=387'>388</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=388'>389</a>\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m--> <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=389'>390</a>\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps)\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=390'>391</a>\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=391'>392</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py:348\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=345'>346</a>\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=346'>347</a>\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=347'>348</a>\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=348'>349</a>\u001b[0m     cloned_transformer,\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=349'>350</a>\u001b[0m     X,\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=350'>351</a>\u001b[0m     y,\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=351'>352</a>\u001b[0m     \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=352'>353</a>\u001b[0m     message_clsname\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPipeline\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=353'>354</a>\u001b[0m     message\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_message(step_idx),\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=354'>355</a>\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params_steps[name],\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=355'>356</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=356'>357</a>\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=357'>358</a>\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=358'>359</a>\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=359'>360</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[0;32m~/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/joblib/memory.py:352\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/joblib/memory.py?line=350'>351</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/joblib/memory.py?line=351'>352</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=890'>891</a>\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=891'>892</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m--> <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=892'>893</a>\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39;49mfit_transform(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=893'>894</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/pipeline.py?line=894'>895</a>\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:671\u001b[0m, in \u001b[0;36mColumnTransformer.fit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py?line=668'>669</a>\u001b[0m \u001b[39m# set n_features_in_ attribute\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py?line=669'>670</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_n_features(X, reset\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py?line=670'>671</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_transformers()\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py?line=671'>672</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_column_callables(X)\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py?line=672'>673</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_remainder(X)\n",
      "File \u001b[0;32m~/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py:324\u001b[0m, in \u001b[0;36mColumnTransformer._validate_transformers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py?line=320'>321</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformers:\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py?line=321'>322</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py?line=323'>324</a>\u001b[0m names, transformers, _ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformers)\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py?line=325'>326</a>\u001b[0m \u001b[39m# validate names\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/pessini/Dropbox/Data-Science/moby-bikes/conda-env/lib/python3.9/site-packages/sklearn/compose/_column_transformer.py?line=326'>327</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_names(names)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "predictors = ['temp_r','rhum','holiday','dayofweek','timesofday','wind_speed_group','rainfall_intensity','peak','hour','working_day']\n",
    "# random forest model\n",
    "params_rf = {'n_estimators': 100, \n",
    "             'max_depth': 20, \n",
    "             'random_state': 0, \n",
    "             'min_samples_split' : 5,\n",
    "             'n_jobs': -1}\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor(predictors)),\n",
    "    ('model', RandomForestRegressor(**params_rf, criterion='mae'))\n",
    "])\n",
    "# pipe_rf.fit(X_train[predictors], y_train)\n",
    "train_rsme, val_rsme, train_mae, val_mae = get_train_val_score(pipe_rf, predictors)\n",
    "exp_rf_regr = et.Experiment('Random Forest', predictors=predictors, hyperparameters=pipe_rf['model'].get_params(),\n",
    "                               score=get_metrics_to_Experiment(), notes='Added all predictors and using preprocessing')\n",
    "experiment_tracker.add_experiment(exp_rf_regr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "display(pipe_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(pipe_rf['model'], predictors,'Random Forest Feature Importances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=2022)\n",
    "\n",
    "for n_fold, (train_index, test_index) in enumerate(cv.split(X_train, y_train)):\n",
    "    print('#'*40, f'Fold {n_fold+1} out of {cv.n_splits}', '#'*40)\n",
    "    \n",
    "    # X_train, y_train = X[train_index], y[train_index] # Train data\n",
    "    # X_val, y_val = X[test_index], y[test_index] # Valid data\n",
    "    \n",
    "    # pipe_gbm.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "    #           verbose=250, early_stopping_rounds=50)\n",
    "    \n",
    "    # preds_lgb[test_index] += pipe_gbm.predict(X_val, raw_score=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catboost model\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "# pipe_catboost = Pipeline([\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('model', CatBoostRegressor(verbose=1, n_estimators=100))\n",
    "# ])\n",
    "catboost_model = CatBoostRegressor(n_estimators=5000, depth=3, learning_rate=0.01, loss_function='RMSE')\n",
    "catboost_model.fit(X_train, y_train, cat_features=predictors, eval_set=(X_test, y_test), verbose=250, early_stopping_rounds=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = catboost_model.predict(df_test[predictors])\n",
    "y_hat = pd.DataFrame(y_test_pred, columns=['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mean_absolute_error(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM model\n",
    "params_lightgbm = {'n_estimators': 5000,\n",
    "                   'objective': 'l1',\n",
    "                   'learning_rate': 0.01, \n",
    "                   'subsample': 0.7,\n",
    "                   'verbosity': -1,\n",
    "                   'feature_fraction': 0.5,\n",
    "                   'bagging_fraction': 0.5,\n",
    "                   'bagging_freq': 20,\n",
    "                   'importance_type': 'gain'\n",
    "                   }\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_gbm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LGBMRegressor(**params_lightgbm))\n",
    "])\n",
    "pipe_gbm.fit(X_train, y_train)\n",
    "\n",
    "#Plotting features importance\n",
    "feature_imp = pd.DataFrame(sorted(zip(pipe_gbm['model'].feature_importances_,X_train.columns)), \n",
    "                           columns=['Value','Feature'])\n",
    "scaler_ft = MinMaxScaler()\n",
    "feature_imp['Value'] = scaler_ft.fit_transform(feature_imp['Value'].values.reshape(-1,1));\n",
    "\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('LightGBM Features Importance')\n",
    "locs, labels = plt.xticks()\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolation problem \n",
    "\n",
    "When using a Random Forest Regressor, the predicted values are never outside the training set values for the target variable. If it is tasked with the problem of predicting for values not previously seen, it will always predict an average of the values seen previously. Obviously the average of a sample can not fall outside the highest and lowest values in the sample. \n",
    "\n",
    "The Random Forest Regressor is unable to discover trends that would enable it in extrapolating values that fall outside the training set. When faced with such a scenario, the regressor assumes that the prediction will fall close to the maximum value in the training set. \n",
    "\n",
    "\n",
    "### Potential solutions\n",
    "\n",
    "Ok, so how can you deal with this extrapolation problem?\n",
    "\n",
    "There are a couple of options:\n",
    "\n",
    "- Use a linear model such as SVM regression, Linear Regression, etc\n",
    "- Build a deep learning model because neural nets are able to extrapolate (they are basically stacked linear regression models on steroids)\n",
    "- Combine predictors using [stacking](https://scikit-learn.org/stable/auto_examples/ensemble/plot_stack_predictors.html). For example, you can create a stacking regressor using a Linear model and a Random Forest Regressor. \n",
    "- Use modified versions of random forest\n",
    "\n",
    "One of such extensions is [Regression-Enhanced Random Forests](https://arxiv.org/pdf/1904.10416.pdf) (RERFs). The authors of this paper propose a technique borrowed from the strengths of penalized parametric regression to give better results in extrapolation problems.\n",
    "\n",
    "Specifically, there are two steps to the process:\n",
    "\n",
    "run Lasso before Random Forest, \n",
    "train a Random Forest on the residuals from Lasso. \n",
    "Since Random Forest is a fully nonparametric predictive algorithm, it may not efficiently incorporate known relationships between the response and the predictors. The response values are the observed values Y1, . . . , Yn  from the training data. RERFs are able to incorporate known relationships between the responses and the predictors which is another benefit of using Regression-Enhanced Random Forests for regression problems.\n",
    "\n",
    "Source: https://neptune.ai/blog/random-forest-regression-when-does-it-fail-and-why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest model\n",
    "params_rf = {'n_estimators': 1000, \n",
    "             'max_depth': 20, \n",
    "             'random_state': 0, \n",
    "             'min_samples_split' : 5,\n",
    "             'n_jobs': -1}\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(**params_rf, criterion='mae'))\n",
    "])\n",
    "pipe_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_rf['model'].feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting features importance\n",
    "feature_imp = pd.DataFrame(sorted(zip(pipe_rf['model'].feature_importances_,X_train.columns)), \n",
    "                           columns=['Value','Feature'])\n",
    "scaler_ft = MinMaxScaler()\n",
    "feature_imp['Value'] = scaler_ft.fit_transform(feature_imp['Value'].values.reshape(-1,1));\n",
    "\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('Random Forest Features Importance')\n",
    "locs, labels = plt.xticks()\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_importance = permutation_importance(pipe_rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "sorted_idx\n",
    "# plt.barh(feature_imp[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "# plt.xlabel(\"Permutation Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest model\n",
    "params_rf = {'n_estimators': 1000, \n",
    "             'max_depth': 20, \n",
    "             'random_state': 0, \n",
    "             'min_samples_split' : 5,\n",
    "             'n_jobs': -1}\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(**params_rf, criterion='mae'))\n",
    "])\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "\n",
    "#Plotting features importance\n",
    "feature_imp = pd.DataFrame(sorted(zip(pipe_gbm['model'].feature_importances_,X_train.columns)), \n",
    "                           columns=['Value','Feature'])\n",
    "scaler_ft = MinMaxScaler()\n",
    "feature_imp['Value'] = scaler_ft.fit_transform(feature_imp['Value'].values.reshape(-1,1));\n",
    "\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('Random Forest Features Importance')\n",
    "locs, labels = plt.xticks()\n",
    "plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest model\n",
    "params_rf = {'n_estimators': 1000, \n",
    "             'max_depth': 20, \n",
    "             'random_state': 0, \n",
    "             'min_samples_split' : 5, \n",
    "             'n_jobs': -1}\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(**params_rf))\n",
    "])\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "# y_train_pred = pipe_rf.predict(X_train)\n",
    "# y_test_pred = pipe_rf.predict(X_test)\n",
    "\n",
    "# print_evalmetrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_svr = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', SVR(kernel='poly',gamma='scale',C=100))\n",
    "])\n",
    "pipe_svr.fit(X_train, y_train)\n",
    "y_train_pred = pipe_svr.predict(X_train)\n",
    "y_test_pred = pipe_svr.predict(X_test)\n",
    "\n",
    "print_evalmetrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# define the model cross-validation configuration\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "# evaluate the pipeline using cross validation and calculate MAE\n",
    "scores = cross_val_score(pipe_svr, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "# convert MAE scores to positive values\n",
    "scores = absolute(scores)\n",
    "# summarize the model performance\n",
    "print('MAE: %.3f (%.3f)' % (mean(scores), std(scores)))\n",
    "y_test_pred = pipe_svr.predict(X_test)\n",
    "print_evalmetrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = pd.DataFrame()\n",
    "predicted_values['real'] = y_test\n",
    "predicted_values['predicted'] = y_test_pred\n",
    "\n",
    "predicted_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GradientBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GBM model\n",
    "params_gbm = {'n_estimators': 150, \n",
    "              'max_depth': 5, \n",
    "              'random_state': 0, \n",
    "              'min_samples_leaf' : 10, \n",
    "              'learning_rate': 0.01, \n",
    "              'subsample': 0.7, \n",
    "              'loss': 'ls'}\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_gbm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(**params_gbm))\n",
    "])\n",
    "pipe_gbm.fit(X_train, y_train)\n",
    "y_train_pred = pipe_gbm.predict(X_train)\n",
    "y_test_pred = pipe_gbm.predict(X_test)\n",
    "\n",
    "print_evalmetrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM model\n",
    "params_lightgbm = {'n_estimators': 1000, \n",
    "                   'max_depth': 15, \n",
    "                   'random_state': 0, \n",
    "                   'learning_rate': 0.01, \n",
    "                   'subsample': 0.7,\n",
    "                   'num_leaves': 30,\n",
    "                   'metric': 'rmse',\n",
    "                   'n_jobs': 2\n",
    "                   }\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_gbm = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LGBMRegressor(**params_lightgbm))\n",
    "])\n",
    "pipe_gbm.fit(X_train, y_train)\n",
    "y_train_pred = pipe_gbm.predict(X_train)\n",
    "y_test_pred = pipe_gbm.predict(X_test)\n",
    "\n",
    "print_evalmetrics(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -a \"Leandro Pessini\" -n -u -v -iv -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img title=\"GitHub Mark\" src=\"./img/GitHub-Mark-64px.png\" style=\"height: 32px; padding-right: 15px\" alt=\"GitHub Mark\" align=\"left\"> [GitHub repository](https://github.com/pessini/moby-bikes) <br>Author: Leandro Pessini"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e88ab788e4c28b6ebfdd315341ca6b84d0235bda4bdece235b181ca971ce4b33"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
