{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<img title=\"GitHub Octocat\" src='./img/Octocat.jpg' style='height: 60px; padding-right: 15px' alt=\"Octocat\" align=\"left\"> This notebook is part of a GitHub repository: https://github.com/pessini/moby-bikes \n",
    "<br>MIT Licensed\n",
    "<br>Author: Leandro Pessini"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img title=\"Moby Bikes\" src=\"https://i.ytimg.com/vi/-s8er6tHD3o/maxresdefault.jpg\" style=\"height:400px; border-radius: 10px\" alt=\"Moby Bikes\" class=\"img-fluid\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "  <h1 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">&nbsp;Table of Contents:</h1>\n",
    "  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#1\" role=\"tab\" aria-controls=\"profile\">1. Introduction<span class=\"badge badge-primary badge-pill\">1</span></a>\n",
    "  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#2\" role=\"tab\" aria-controls=\"messages\">2. Feature Engineering<span class=\"badge badge-primary badge-pill\">2</span></a>\n",
    "  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"messages\">3. EDA (Exploratory Data Analysis)<span class=\"badge badge-primary badge-pill\">3</span></a>\n",
    "   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"messages\">4. Model<span class=\"badge badge-primary badge-pill\">4</span></a>\n",
    "   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#5\" role=\"tab\" aria-controls=\"messages\">5. Evaluation<span class=\"badge badge-primary badge-pill\">5</span></a>    \n",
    "   <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#6\" role=\"tab\" aria-controls=\"messages\">6. Conclusion<span class=\"badge badge-primary badge-pill\">6</span></a>\n",
    "</div>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"1\"></a>\n",
    "# <p style=\"font-size:100%; text-align:left; color:#444444;\">1- Introduction</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Datasets\n",
    "\n",
    "Dataset provided by [Moby Bikes](https://data.gov.ie/dataset/moby-bikes) through a public [API](https://data.smartdublin.ie/mobybikes-api). \n",
    "\n",
    "Dataset provided by [Met Éireann](https://www.met.ie/) through a public [API](https://data.gov.ie/organization/meteireann). \n",
    "\n",
    "\n",
    "The **aim** of this report is to map how this disease spread throughout the island and provide insights on possible spots and species that might need extra attention from the scientists who investigate this constant threat to resident birds.\n",
    "\n",
    "## Questions\n",
    "\n",
    "- What species have shown to be the most affected with Bird Flu?\n",
    "- What are the most frequent locations where captured birds have been detected with Avian Flu?\n",
    "- November is the month with the highest presence as mentioned? What are the months with the highest proportion of infected birds?\n",
    "- The percentage of infected birds have increased during the years?\n",
    "- What is the proportion of birds targeted with Avian Flu on each Council / County?\n",
    "- Which areas present statistically significant incidence of Bird Flu?\n",
    "\n",
    "\n",
    ">https://mobidev.biz/blog/machine-learning-methods-demand-forecasting-retail"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from conn import mongodb\n",
    "import importlib\n",
    "from pymongo import MongoClient\n",
    "from urllib.parse import quote_plus\n",
    "importlib.reload(mongodb)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'conn.mongodb' from '/Users/pessini/Dropbox/Data-Science/moby-bikes/notebooks/conn/mongodb.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def _connect_mongo(host, port, username, password, db_name):\n",
    "    \"\"\" A util for making a connection to mongo \"\"\"\n",
    "\n",
    "    if username and password:\n",
    "        try:\n",
    "            mongo_uri = f'mongodb://{username}:{quote_plus(password)}@{host}:{port}/{db_name}'\n",
    "            conn = MongoClient(mongo_uri)\n",
    "        except:\n",
    "            print('Could not connect to MongoDB')\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "\n",
    "    return conn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def read_mongo(query={}, collection='', no_id=True):\n",
    "    \"\"\" Read from Mongo and Store into a DataFrame \"\"\"\n",
    "    df = None\n",
    "    try:\n",
    "        # Connect to MongoDB\n",
    "        conn = _connect_mongo(host=mongodb.host, port=mongodb.port, username=mongodb.user_name, password=mongodb.pass_word, db_name=mongodb.db_name)\n",
    "        db = conn.mobybikes # switch to the database\n",
    "\n",
    "        if collection in db.list_collection_names():\n",
    "            \n",
    "            # Make a query to the specific DB and Collection and store into a Dataframe\n",
    "            data = db[collection].find(query)\n",
    "            df =  pd.DataFrame(list(data))\n",
    "            \n",
    "            # Delete the _id\n",
    "            if no_id:\n",
    "                del df['_id']\n",
    "        else:\n",
    "            print(f'Collection {collection} was not found!')\n",
    "            pass\n",
    "\n",
    "        # close mongodb connection\n",
    "        conn.close()\n",
    "    except:\n",
    "        print('Could not query MongoDB')\n",
    "    \n",
    "    return df\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# from bson.objectid import ObjectId\n",
    "\n",
    "# query = {\n",
    "#     '_id': ObjectId('')\n",
    "# }\n",
    "\n",
    "historical_data = read_mongo(collection='historical')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "historical_data.columns = historical_data.columns.str.lower()\n",
    "historical_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "historical_data.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weather Data - Met Éireann\n",
    "\n",
    "About the weather data there are two important decisions. One is about from **which station** the **historical data will be collected** and the other one is about the **frequency of data**, which can be **hourly or daily**."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Station Name: **PHOENIX PARK**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "phoenixpark_weather_hourly = pd.read_csv('../data/raw/hly175.csv')\n",
    "phoenixpark_weather_hourly.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "phoenixpark_weather_daily = pd.read_csv('../data/raw/dly175.csv')\n",
    "phoenixpark_weather_daily.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Station Name: **DUBLIN AIRPORT**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dublin_airport_weather_hourly = pd.read_csv('../data/raw/hly532.csv')\n",
    "dublin_airport_weather_hourly.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Phoenix Park Station vs Dublin Aiport Station\n",
    "Geographically, the station at Phoenix Park would be the most suitable choice but unfortunately, they do not collect Wind information which in Ireland plays an important role when deciding to go cycling or not. For those who are not familiar with Irish weather, it rains a lot and mostly we do not have much choice about it but the wind is something that can prevent you go outside or choose a different kind of transport. Heavy rain is kind of rare on the other hand.\n",
    "\n",
    "## Hourly vs Daily data\n",
    "On this subject, daily data for the business would make more sense but because the weather is so unpredictable in Ireland (it can completely change in an hour), the best option would be hourly especially if we are looking at a historical perspective. For simplicity and better planning, we can always aggregate the predicted results by day."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dublin_airport_weather_hourly['date'] = pd.to_datetime(dublin_airport_weather_hourly['date'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dublin_airport_weather_hourly.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dublin_airport_weather_hourly.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "recent_dubairport_data = dublin_airport_weather_hourly.copy()\n",
    "\n",
    "start_date_hist = datetime(2020, 9, 23) # first day registered\n",
    "end_date_hist = datetime(2021, 8, 31) # last day used as historical data\n",
    "\n",
    "recent_dubairport_data = recent_dubairport_data[(recent_dubairport_data.date >= start_date_hist) & (recent_dubairport_data.date <= end_date_hist)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(dublin_airport_weather_hourly), len(recent_dubairport_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "columns_to_drop = ['ind','ind.1','ind.2','ind.3','vappr','msl','ind.4','wddir','ww','w','sun','vis','clht','clamt']\n",
    "weather_data = recent_dubairport_data.drop(columns=columns_to_drop)\n",
    "weather_data.to_csv('../data/interim/hist_weather_data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weather_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hypothesis\n",
    "\n",
    "Hourly trend: There must be high demand during office timings. Early morning and late evening can have different trend (cyclist) and low demand during 10:00 pm to 4:00 am.\n",
    "\n",
    "Daily Trend: Registered users demand more bike on weekdays as compared to weekend or holiday.\n",
    "\n",
    "Rain: The demand of bikes will be lower on a rainy day as compared to a sunny day. Similarly, higher humidity will cause to lower the demand and vice versa.\n",
    "\n",
    "Temperature: In Ireland, temperature has positive correlation with bike demand.\n",
    "\n",
    "Time: Total demand should have higher contribution of registered user as compared to casual because registered user base would increase over time.\n",
    "\n",
    "Traffic: It can be positively correlated with Bike demand. Higher traffic may force people to use bike as compared to other road transport medium like car, taxi etc.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"2\"></a>\n",
    "# <p style=\"font-size:100%; text-align:left; color:#444444;\">2- Feature Engineering</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### New Features\n",
    "- date (yyyy-mm-dd)\n",
    "- month\n",
    "- hour\n",
    "- workingday\n",
    "- holiday\n",
    "- season\n",
    "- battery_start\n",
    "- battery_end\n",
    "- path? (multi polygon)\n",
    "- rental_duration\n",
    "\n",
    "\n",
    "The number of rentals each hour will be aggregate later with a new feature `count`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rentals_data = historical_data.drop(['harvesttime','ebikestateid'], axis=1).copy()\n",
    "rentals_data[[\"lastgpstime\", \"lastrentalstart\"]] = rentals_data[[\"lastgpstime\", \"lastrentalstart\"]].apply(pd.to_datetime)\n",
    "\n",
    "rentals_data = rentals_data.astype({'battery': np.int16}, errors='ignore') # errors ignore to keep missing values (not throwing error)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rentals_data.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Rentals' information\n",
    "\n",
    "- `coordinates`: converting latitude and longitude to an array to store a GeoJSON object *MultiPoint* \n",
    "- `start_battery`: getting the battery status when the rental started\n",
    "- `lastgpstime`: new variable that will only store the last record when grouping rentals"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def feat_eng(x):\n",
    "    d = {}\n",
    "    d['coordinates'] = x[['latitude','longitude']].values.tolist()\n",
    "    d['start_battery'] = list(x['battery'])[-1] # get the first battery status (when rental started)\n",
    "    d['lastgpstime'] = list(x['lastgpstime'])[0] # get the last gpstime (previously sorted)\n",
    "    \n",
    "    return pd.Series(d, index=['coordinates', 'start_battery', 'lastgpstime'])\n",
    "\n",
    "# also sorting data by lastgpstime\n",
    "grouped_rentals = rentals_data.sort_values(\"lastgpstime\", ascending=False).groupby(['lastrentalstart', 'bikeid']).apply(feat_eng).reset_index()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grouped_rentals.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Date and time - new features\n",
    "- `rental_date`\n",
    "- `rental_month`\n",
    "- `rental_hour`\n",
    "- `holiday`\n",
    "- `workingday`\n",
    "- `season`: (1 = Spring, 2 = Summer, 3 = Fall, 4 = Winter)\n",
    "- *`duration`: duration of the rental\n",
    "\n",
    "\\* **Assumption**: that the duration of the rental is $ LastGPSTime - RentalStart $"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grouped_rentals['rental_date'] = pd.to_datetime(grouped_rentals['lastrentalstart'].dt.date)\n",
    "grouped_rentals['rental_month'] = grouped_rentals['lastrentalstart'].dt.month\n",
    "grouped_rentals['rental_hour'] = grouped_rentals['lastrentalstart'].dt.hour"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grouped_rentals.info()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# time of rental in minutes (lastgpstime - rental-start)\n",
    "grouped_rentals['duration'] = (grouped_rentals['lastgpstime'] - grouped_rentals['lastrentalstart']) / pd.Timedelta(minutes=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A few GPS records have frozen and stopped sending the accurate data back. About 345 records which would lead to a bias duration of rentals.\n",
    "\n",
    "To prevent any inaccurate information these records will be set as `NaN`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grouped_rentals['duration'] = np.where(grouped_rentals['duration'] < 0, np.NaN, grouped_rentals['duration'])\n",
    "len(grouped_rentals[ np.isnan(grouped_rentals['duration']) ])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bank Holidays"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "qry_bh = {\n",
    "    'type': 'National holiday'\n",
    "}\n",
    "\n",
    "bank_holidays = read_mongo(query=qry_bh, collection='irishcalendar')\n",
    "bank_holidays.drop(['country', 'type'], axis=1, inplace=True)\n",
    "bank_holidays['date'] = pd.DatetimeIndex(bank_holidays['date'].apply(pd.to_datetime))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# holiday\n",
    "grouped_rentals['holiday'] = grouped_rentals['rental_date'].isin(bank_holidays['date'])\n",
    "\n",
    "# day of the week\n",
    "grouped_rentals['dayofweek'] = grouped_rentals['rental_date'].dt.dayofweek\n",
    "\n",
    "# working day (Monday=0, Sunday=6)\n",
    "grouped_rentals['working_day'] = grouped_rentals['dayofweek'] < 5 # from 0 to 4 or monday to friday"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Seasons"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "Y = 2000 # dummy leap year to allow input X-02-29 (leap day)\n",
    "seasons = [(3, (datetime(Y,  1,  1),  datetime(Y,  3, 20))),\n",
    "           (0, (datetime(Y,  3, 21),  datetime(Y,  6, 20))),\n",
    "           (1, (datetime(Y,  6, 21),  datetime(Y,  9, 22))),\n",
    "           (2, (datetime(Y,  9, 23),  datetime(Y, 12, 20))),\n",
    "           (3, (datetime(Y, 12, 21),  datetime(Y, 12, 31)))]\n",
    "\n",
    "def get_season(date: pd.DatetimeIndex) -> int:\n",
    "    '''\n",
    "        Receives a date and returns the corresponded season\n",
    "        0 - Spring | 1 - Summer | 2 - Autumn | 3 - Winter\n",
    "        Vernal equinox(about March 21): day and night of equal length, marking the start of spring\n",
    "        Summer solstice (June 20 or 21): longest day of the year, marking the start of summer\n",
    "        Autumnal equinox(about September 23): day and night of equal length, marking the start of autumn\n",
    "        Winter solstice (December 21 or 22): shortest day of the year, marking the start of winter\n",
    "    '''\n",
    "    date = date.replace(year=Y)\n",
    "    return next(season for season, (start, end) in seasons if start <= date <= end)\n",
    "\n",
    "\n",
    "grouped_rentals['season'] = grouped_rentals.rental_date.map(get_season)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grouped_rentals.groupby('season').size()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grouped_rentals.isnull().sum()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Battery"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grouped_rentals['start_battery'] = pd.to_numeric(grouped_rentals['start_battery'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "grouped_rentals[grouped_rentals['start_battery'] > 100]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "From the battery records there is a few cases that we can consider. Only one record has `> 100` and a few negatives ones. To simplify the analysis the records will be normalized with values between `0 > x > 100`.\n",
    "\n",
    "All missing values (*n=571*) will not be transformed as it could be only malfunction issue when transmiting the data and it could mislead the analysis."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# normalize battery status between 0 > x < 100\n",
    "grouped_rentals['start_battery'] = abs(grouped_rentals['start_battery'])\n",
    "grouped_rentals.loc[grouped_rentals['start_battery'] > 100, 'start_battery'] = 100"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "new_rentals = grouped_rentals.copy()\n",
    "new_rentals.to_csv('../data/interim/new_features_rentals.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Humidity\n",
    "\n",
    "Attempt to create a new feature called `Humidity` to avoid Multicollinearity."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### We need different humidity quantities\n",
    "\n",
    "The problem with relative humidity is that, by itself, it doesn’t really tell you how humid it is.\n",
    "\n",
    "- **Relative Humidity** – This quantity tells us how close the conditions are to saturation, when condensation of water vapor can occur. The interaction of porous materials with water vapor increases with increasing RH. The chance of growing mold increases with increasing RH, 70% usually given as the threshold to stay below.\n",
    "- **Dew Point Temperature** – This temperature scales with the amount of water vapor. As more water vapor enters a volume, the dew point goes up. If the air in your crawl space, for example, has a dew point of 75° F, you’re probably going to find condensation somewhere. Look at the water pipes, poorly insulated ducts, and uninsulated duct boots.\n",
    "- **Wet Bulb Temperature** – If dew point is the temperature of condensation, wet bulb is the temperature of evaporation. Same concept; different direction. This one’s important for cooling our bodies.\n",
    "\n",
    "Once you get a handle on these three quantities, you’ll have a pretty good understanding of humidity.\n",
    "\n",
    "> https://www.energyvanguard.com/blog/problem-with-relative-humidity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weather_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "weather_data['rhum'] = pd.to_numeric(weather_data['rhum'],errors = 'coerce')\n",
    "weather_data['wetb'] = pd.to_numeric(weather_data['wetb'],errors = 'coerce')\n",
    "weather_data['dewpt'] = pd.to_numeric(weather_data['dewpt'],errors = 'coerce')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variance Inflation Factor(VIF)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_vif = weather_data.copy().drop('date', axis=1)\n",
    "#Calculate VIF for each variable in the new data frame\n",
    "vif = pd.DataFrame()\n",
    "vif[\"features\"] = test_vif.columns\n",
    "vif[\"vif_value\"] = [variance_inflation_factor(test_vif.values, i) for i in range(test_vif.shape[1])]\n",
    "vif"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To avoid multicolinearity we could create a new feature based on some calculation on those three features related to humidity. Unfortunately, the Forecast API later that will be used in production to predict the demand **does not include Wet Bulb Temperature**.\n",
    "\n",
    "There are a few equations that could applied to calculate Wet Bulb Temperature (eg: Stull formula) but because we do not have which one is used to store this feature on their historical the best would be not use this feature at all."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#weather_data.drop(['rhum', 'wetb', 'dewpt'], axis=1, inplace=True)\n",
    "\n",
    "weather_data.drop(['wetb', 'dewpt'], axis=1, inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combining Rentals and Weather data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rentals = new_rentals.copy()\n",
    "weather = weather_data.copy()\n",
    "\n",
    "weather['rental_date'] = pd.to_datetime(weather['date'].dt.date)\n",
    "weather['rental_hour'] = weather['date'].dt.hour"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rentals.groupby('season').size()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_data = pd.merge(rentals, weather, on=['rental_date', 'rental_hour'])\n",
    "all_data.to_csv('../data/interim/all_data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_data.groupby('season').size()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"3\"></a>\n",
    "# <p style=\"font-size:100%; text-align:left; color:#444444;\">3- Exploratory Data Analysis (EDA)</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Hourly trend\n",
    "- Holiday\n",
    "- Working_day\n",
    "- Season"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hourly_rentals = all_data.copy()\n",
    "count_hourly_rentals = hourly_rentals.groupby(['rental_date', 'rental_hour']).size().reset_index(name='count')\n",
    "columns_to_drop = ['lastrentalstart','bikeid','coordinates','start_battery','lastgpstime','rental_month','duration','date']\n",
    "hourly_rentals = hourly_rentals.drop(columns_to_drop, axis=1).drop_duplicates(subset=['rental_date', 'rental_hour'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hourly_rentals.shape, count_hourly_rentals.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hourly_data = pd.merge(hourly_rentals, count_hourly_rentals, on=['rental_date','rental_hour'])\n",
    "hourly_data.to_csv('../data/interim/hourly_data.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "hourly_data.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Viz\n",
    "\n",
    "- Hour x Count by Season\n",
    "- Hour x Count by day of week\n",
    "- Hour x Count by temperature\n",
    "- Battery\n",
    "- Duration of rentals"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "season_freq = hourly_data.groupby(['season'])['count'].agg('sum').reset_index(name='count')\n",
    "season_freq"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 0 - Spring | 1 - Summer | 2 - Autumn | 3 - Winter\n",
    "season_map = {0:'Spring', 1:'Summer', 2:'Fall', 3:'Winter'}\n",
    "season_freq = hourly_data.groupby(['season', 'rental_hour'])['count'].agg('sum').reset_index(name='count')\n",
    "season_freq['season'] = season_freq['season'].map(lambda d : season_map[d])\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "sns.pointplot(x=season_freq[\"rental_hour\"], y=season_freq[\"count\"], hue=season_freq[\"season\"], ax=ax)\n",
    "ax.set(xlabel='Hour Of The Day', ylabel='Rentals Count', title=\"Number of Rentals By Hour Of The Day Across Seasons\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Note\n",
    "\n",
    "Summer data is very low due to the last data collect is June 30th and the summer have just started. **Decided to not use the season as a predictor** for now and after the changing of season the model will be rebuild adding this feature."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "day_of_week_map = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\n",
    "day_of_week = hourly_data.groupby(['dayofweek', 'rental_hour'])['count'].agg('sum').reset_index(name='count')\n",
    "day_of_week['dayofweek'] = day_of_week['dayofweek'].map(lambda d : day_of_week_map[d])\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "sns.pointplot(x=day_of_week[\"rental_hour\"], y=day_of_week[\"count\"], hue=day_of_week[\"dayofweek\"], ax=ax)\n",
    "ax.set(xlabel='Hour Of The Day', ylabel='Rentals Count', title=\"Number of Rentals By Hour Of The Day Across Days of Week\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "battery_dist = all_data.copy()\n",
    "\n",
    "def group_battery_status(df):\n",
    "\n",
    "    bins= [0,30,50,80,100]\n",
    "    labels = ['< 30%','30% - 50%','50% - 80%','> 80%']\n",
    "    battery_dist['battery_status'] = pd.cut(battery_dist['start_battery'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    s = battery_dist.battery_status\n",
    "    counts = s.value_counts()\n",
    "    percent = s.value_counts(normalize=True)\n",
    "    percent100 = s.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\n",
    "    \n",
    "    return pd.DataFrame({'counts': counts, 'per': percent, 'per100': percent100}, labels)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "group_battery_status(battery_dist)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.histplot(data=new_rentals, x='start_battery', kde=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "all_data['duration'].mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, axes = plt.subplots(nrows=2,ncols=2)\n",
    "fig.set_size_inches(12, 10)\n",
    "sns.boxplot(data=hourly_data,y=\"count\",orient=\"v\",ax=axes[0][0])\n",
    "sns.boxplot(data=hourly_data,y=\"count\",x=\"season\",orient=\"v\",ax=axes[0][1])\n",
    "sns.boxplot(data=hourly_data,y=\"count\",x=\"rental_hour\",orient=\"v\",ax=axes[1][0])\n",
    "sns.boxplot(data=hourly_data,y=\"count\",x=\"working_day\",orient=\"v\",ax=axes[1][1])\n",
    "\n",
    "axes[0][0].set(ylabel='Count',title=\"Box Plot On Count\")\n",
    "axes[0][1].set(xlabel='Season', ylabel='Count',title=\"Box Plot On Count Across Season\")\n",
    "axes[1][0].set(xlabel='Hour Of The Day', ylabel='Count',title=\"Box Plot On Count Across Hour Of The Day\")\n",
    "axes[1][1].set(xlabel='Working Day', ylabel='Count',title=\"Box Plot On Count Across Working Day\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corrMatt = hourly_data[[\"temp\",\"rain\",\"wdsp\",'rhum',\"count\"]].corr()\n",
    "mask = np.array(corrMatt)\n",
    "mask[np.tril_indices_from(mask)] = False\n",
    "fig,ax= plt.subplots()\n",
    "fig.set_size_inches(8,8)\n",
    "sns.heatmap(corrMatt, mask=mask,vmax=.8, square=True,annot=True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ax.scatter(hourly_data['count'], hourly_data['temp'])\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(22,7))\n",
    "gs = fig.add_gridspec(1, 3)\n",
    "ax0 = fig.add_subplot(gs[0, 0])\n",
    "ax1 = fig.add_subplot(gs[0, 1])\n",
    "ax2 = fig.add_subplot(gs[0, 2])\n",
    "\n",
    "sns.distplot(hourly_data['temp'],ax=ax0)\n",
    "sns.distplot(hourly_data['rain'],ax=ax1)\n",
    "sns.distplot(hourly_data['wdsp'],ax=ax2)\n",
    "\n",
    "ax0.set(xlabel='Temperature',ylabel='Count',title=\"Distribution - Temperature\")\n",
    "ax1.set(xlabel='Rain', ylabel='Count',title=\"Distribution - Rain\")\n",
    "ax2.set(xlabel='Wind Speed', ylabel='Count',title=\"Distribution - Wind Speed\")\n",
    "\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id=\"4\"></a>\n",
    "# <p style=\"font-size:100%; text-align:left; color:#444444;\">4- Models</p>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting dataset in train and test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = hourly_data.copy()\n",
    "df = df.astype({'holiday': 'category', 'dayofweek': 'category', 'working_day': 'category'})\n",
    "predictors = ['rain','temp','wdsp','rhum','holiday','dayofweek','working_day']\n",
    "\n",
    "X = df[[c for c in df.columns if c in predictors]]\n",
    "y = df.pop('count')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split columns in categorical and numerical to use preprocessing pipeline"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_vars = [c for c in df.select_dtypes(include=['number']).columns if c in predictors] # list comprehension to select only predictors features\n",
    "cat_vars = df.select_dtypes(include=['category']).columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing Pipelines"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Define categorical pipeline\n",
    "cat_pipe = Pipeline([\n",
    "    #('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "# Define numerical pipeline\n",
    "num_pipe = Pipeline([\n",
    "    #('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine categorical and numerical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_pipe, cat_vars),\n",
    "    ('num', num_pipe, num_vars)\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "pipe.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Predict training data\n",
    "y_train_pred = pipe.predict(X_train)\n",
    "print(f\"Predictions on training data: {y_train_pred}\")\n",
    "\n",
    "# Predict test data\n",
    "y_test_pred = pipe.predict(X_test)\n",
    "print(f\"Predictions on test data: {y_test_pred}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def adjusted_r2_score(X, r2_score):\n",
    "    return 1 - ( 1- r2_score ) * ( len(X) - 1 ) / ( len(X) - X.shape[1] - 1 )\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_test_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_test_pred)) \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2:', metrics.r2_score(y_test, y_test_pred))\n",
    "print('Adjusted R^2:', adjusted_r2_score(X_test, metrics.r2_score(y_test, y_test_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ridge\n",
    "[Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients with l2 regularization.\n",
    "\n",
    "### Lasso\n",
    "The [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) is a linear model that estimates sparse coefficients with l1 regularization.\n",
    "\n",
    "### ElasticNet\n",
    "[Elastic-Net](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet) is a linear regression model trained with both l1 and l2 -norm regularization of the coefficients."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ridge Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_ridge = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', Ridge())\n",
    "])\n",
    "pipe_ridge.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_train_pred = pipe_ridge.predict(X_train)\n",
    "y_test_pred = pipe_ridge.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_test_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_test_pred)) \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2:', metrics.r2_score(y_test, y_test_pred))\n",
    "print('Adjusted R^2:', adjusted_r2_score(X_test, metrics.r2_score(y_test, y_test_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lasso Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_lasso = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', linear_model.Lasso(alpha=0.1))\n",
    "])\n",
    "pipe_lasso.fit(X_train, y_train)\n",
    "y_train_pred = pipe_lasso.predict(X_train)\n",
    "y_test_pred = pipe_lasso.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_test_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_test_pred)) \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2:', metrics.r2_score(y_test, y_test_pred))\n",
    "print('Adjusted R^2:', adjusted_r2_score(X_test, metrics.r2_score(y_test, y_test_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Elastic-Net"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe_elastic = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', ElasticNet())\n",
    "])\n",
    "pipe_elastic.fit(X_train, y_train)\n",
    "y_train_pred = pipe_elastic.predict(X_train)\n",
    "y_test_pred = pipe_elastic.predict(X_test)\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_test_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_test_pred)) \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))\n",
    "print('R^2:', metrics.r2_score(y_test, y_test_pred))\n",
    "print('Adjusted R^2:', adjusted_r2_score(X_test, metrics.r2_score(y_test, y_test_pred)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img title=\"GitHub Mark\" src=\"./img/GitHub-Mark-64px.png\" style=\"height: 32px; padding-right: 15px\" alt=\"GitHub Mark\" align=\"left\"> [GitHub repository](https://github.com/pessini/moby-bikes) <br>Author: Leandro Pessini"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit (conda)"
  },
  "interpreter": {
   "hash": "4c4aaca6d94e52f1ce9cc908cefca5aa099f069e3c4bc9eaa1b64646fc459322"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}